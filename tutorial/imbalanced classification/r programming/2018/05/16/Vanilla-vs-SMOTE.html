<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Vanilla vs SMOTE Flavored Imbalanced Classification | Patrick D Mobley</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Vanilla vs SMOTE Flavored Imbalanced Classification" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A companion notebook to Imbalanced Classification with mlr, here we compare non-SMOTE and SMOTE modeling using logistic regression, decision trees, and randomForest." />
<meta property="og:description" content="A companion notebook to Imbalanced Classification with mlr, here we compare non-SMOTE and SMOTE modeling using logistic regression, decision trees, and randomForest." />
<link rel="canonical" href="https://patdmob.github.io/blog/tutorial/imbalanced%20classification/r%20programming/2018/05/16/Vanilla-vs-SMOTE.html" />
<meta property="og:url" content="https://patdmob.github.io/blog/tutorial/imbalanced%20classification/r%20programming/2018/05/16/Vanilla-vs-SMOTE.html" />
<meta property="og:site_name" content="Patrick D Mobley" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-05-16T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://patdmob.github.io/blog/tutorial/imbalanced%20classification/r%20programming/2018/05/16/Vanilla-vs-SMOTE.html","@type":"BlogPosting","headline":"Vanilla vs SMOTE Flavored Imbalanced Classification","dateModified":"2018-05-16T00:00:00-05:00","datePublished":"2018-05-16T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://patdmob.github.io/blog/tutorial/imbalanced%20classification/r%20programming/2018/05/16/Vanilla-vs-SMOTE.html"},"description":"A companion notebook to Imbalanced Classification with mlr, here we compare non-SMOTE and SMOTE modeling using logistic regression, decision trees, and randomForest.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://patdmob.github.io/blog/feed.xml" title="Patrick D Mobley" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Patrick D Mobley</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Vanilla vs SMOTE Flavored Imbalanced Classification</h1><p class="page-description">A companion notebook to Imbalanced Classification with mlr, here we compare non-SMOTE and SMOTE modeling using logistic regression, decision trees, and randomForest.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2018-05-16T00:00:00-05:00" itemprop="datePublished">
        May 16, 2018
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      20 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#tutorial">tutorial</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#imbalanced classification">imbalanced classification</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#R programming">R programming</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="outline">Outline</h2>

<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#setup">Setup</a></li>
  <li><a href="#benchmarking-logistic-regression">Benchmarking Logistic Regression</a></li>
  <li><a href="#benchmarking-decision-tree">Benchmarking Decision Tree</a></li>
  <li><a href="#benchmarking-randomforest">Benchmarking randomForest</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>This is a companion notebook to <a href="../15/Imbalanced-Classification-with-mlr.html"><em>Imbalanced Classification with mlr</em></a>. In this notebook, we investigate whether SMOTE actually improves model performance. For clarity, non-SMOTE models are referred to as <em>“vanilla”</em> models. We compare these two flavors (vanilla and SMOTE) using <strong>logistic regression</strong>, <strong>decision trees</strong>, and <strong>randomForest</strong>. We also consider how tuning model operating thresholds and tuning SMOTE parameters impact the results.</p>

<p>If you must know, I <em>had</em> to make this. We kept debating on the effectiveness of techniques like SMOTE during my lunch break. Eventually, my curiosity won out and here we are. Does SMOTE work? Keep reading to find out! Or just skip to the conclusion.</p>

<p>For more information about this algorithm, check out the <a href="https://arxiv.org/abs/1106.1813">original paper</a>. Or if you’re looking for a visual explanation, <a href="https://limnu.com/smote-visualization-for-data-science/">this post</a> does a good job.</p>

<p>The findings in this notebook represent observed trends but actual results may vary. Additionally, different datasets may respond differently to SMOTE. These findings are not verified by the FDA. ;)</p>

<p>This work was part of a one month PoC for an Employee Attrition Analytics project at Honeywell International. I presented this notebook at a Honeywell internal data science meetup group and received permission to post it publicly. I would like to thank Matt Pettis (Managing Data Scientist), Nabil Ahmed (Solution Architect), Kartik Raval (Data SME), and Jason Fraklin (Business SME). Without their mentorship and contributions, this project would not have been possible.</p>

<h3 id="a-quick-refresh-on-performance-measures">A Quick Refresh on Performance Measures</h3>

<p>There are lots of performance measures to choose from for classification problems. We’ll look at a few to compare these models.</p>

<h4 id="accuracy">Accuracy</h4>

<p>is the percentage of correctly classified instances. However, if the majority class makes up 99% of the data, then it is easy to get an accuracy of 99% by always predicting the majority class. For this reason, accuracy is not a good measure for imbalanced classification problems. 1 - ACC results in the misclassification error or error rate.</p>

<p><img src="/blog/images/VanillaVsSMOTE_files/Accuracy.png" alt="" /></p>

<h4 id="balanced-accuracy">Balanced Accuracy</h4>

<p>on the other hand, gives equal weight to the relative proportions of negative and positive class instances. If a model predicts only one class, the best balanced accuracy it could receive is 50%. 1 - BAC results in the balanced error rate.</p>

<p><img src="/blog/images/VanillaVsSMOTE_files/BalancedAccuracy.png" alt="" /></p>

<h4 id="f1-score">F1 Score</h4>

<p>is the harmonic mean of precision and recall. A perfect model has a precision and recall of 1 resulting in an F1 score of 1. For all other models, there exists a tradeoff between precision and recall. F1 is a measure that helps us to judge how much of the tradeoff is worthwhile.</p>

<p><img src="/blog/images/VanillaVsSMOTE_files/F1.png" alt="" /></p>

<p>or</p>

<p><img src="/blog/images/VanillaVsSMOTE_files/F1(2).png" alt="" /></p>

<h2 id="setup">Setup</h2>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Libraries</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">tidyverse</span><span class="p">)</span><span class="w">    </span><span class="c1"># Data manipulation</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">mlr</span><span class="p">)</span><span class="w">          </span><span class="c1"># Modeling framework</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">parallelMap</span><span class="p">)</span><span class="w">  </span><span class="c1"># Parallelization  </span><span class="w">

</span><span class="c1"># Parallelization</span><span class="w">
</span><span class="n">parallelStartSocket</span><span class="p">(</span><span class="n">parallel</span><span class="o">::</span><span class="n">detectCores</span><span class="p">())</span><span class="w">

</span><span class="c1"># Loading Data</span><span class="w">
</span><span class="n">source</span><span class="p">(</span><span class="s2">"prep_EmployeeAttrition.R"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h4 id="defining-the-task">Defining the Task</h4>

<p>As before, we define the Task at hand: predicting attrition up to four weeks out.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tsk_4wk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeClassifTask</span><span class="p">(</span><span class="n">id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"4 week prediction"</span><span class="p">,</span><span class="w">
                       </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">select</span><span class="p">(</span><span class="o">-</span><span class="nf">c</span><span class="p">(</span><span class="o">!!</span><span class="w"> </span><span class="n">exclude</span><span class="p">)),</span><span class="w">
                       </span><span class="n">target</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Left4wk"</span><span class="p">,</span><span class="w">  </span><span class="c1"># Must be a factor variable</span><span class="w">
                       </span><span class="n">positive</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Left"</span><span class="w">
                       </span><span class="p">)</span><span class="w">
</span><span class="n">tsk_4wk</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mergeSmallFactorLevels</span><span class="p">(</span><span class="n">tsk_4wk</span><span class="p">)</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">5456</span><span class="p">)</span><span class="w">
</span><span class="n">ho_4wk</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">makeResampleInstance</span><span class="p">(</span><span class="s2">"Holdout"</span><span class="p">,</span><span class="w"> </span><span class="n">tsk_4wk</span><span class="p">,</span><span class="w"> </span><span class="n">stratify</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">   </span><span class="c1"># Default 1/3rd</span><span class="w">
</span><span class="n">tsk_train_4wk</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subsetTask</span><span class="p">(</span><span class="n">tsk_4wk</span><span class="p">,</span><span class="w"> </span><span class="n">ho_4wk</span><span class="o">$</span><span class="n">train.inds</span><span class="p">[[</span><span class="m">1</span><span class="p">]])</span><span class="w">
</span><span class="n">tsk_test_4wk</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subsetTask</span><span class="p">(</span><span class="n">tsk_4wk</span><span class="p">,</span><span class="w"> </span><span class="n">ho_4wk</span><span class="o">$</span><span class="n">test.inds</span><span class="p">[[</span><span class="m">1</span><span class="p">]])</span><span class="w">
</span></code></pre></div></div>

<h4 id="defining-the-learners">Defining the Learners</h4>

<p>Here we define 3 separate learner lists. Each contains the model with and without SMOTE.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rate</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">18</span><span class="w">
</span><span class="n">neighbors</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">5</span><span class="w">

</span><span class="n">logreg_lrns</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="w">
  </span><span class="n">makeLearner</span><span class="p">(</span><span class="s2">"classif.logreg"</span><span class="p">,</span><span class="w"> </span><span class="n">predict.type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"prob"</span><span class="p">)</span><span class="w">
  </span><span class="p">,</span><span class="n">makeSMOTEWrapper</span><span class="p">(</span><span class="n">makeLearner</span><span class="p">(</span><span class="s2">"classif.logreg"</span><span class="p">,</span><span class="w"> </span><span class="n">predict.type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"prob"</span><span class="p">),</span><span class="w">
                   </span><span class="n">sw.rate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rate</span><span class="p">,</span><span class="w"> </span><span class="n">sw.nn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">neighbors</span><span class="p">))</span><span class="w">
</span><span class="n">rpart_lrns</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="w">
  </span><span class="n">makeLearner</span><span class="p">(</span><span class="s2">"classif.rpart"</span><span class="p">,</span><span class="w"> </span><span class="n">predict.type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"prob"</span><span class="p">)</span><span class="w">
  </span><span class="p">,</span><span class="n">makeSMOTEWrapper</span><span class="p">(</span><span class="n">makeLearner</span><span class="p">(</span><span class="s2">"classif.rpart"</span><span class="p">,</span><span class="w"> </span><span class="n">predict.type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"prob"</span><span class="p">),</span><span class="w">
                   </span><span class="n">sw.rate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rate</span><span class="p">,</span><span class="w"> </span><span class="n">sw.nn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">neighbors</span><span class="p">))</span><span class="w">
</span><span class="n">randomForest_lrns</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="w">
  </span><span class="n">makeLearner</span><span class="p">(</span><span class="s2">"classif.randomForest"</span><span class="p">,</span><span class="w"> </span><span class="n">predict.type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"prob"</span><span class="p">)</span><span class="w">
  </span><span class="p">,</span><span class="n">makeSMOTEWrapper</span><span class="p">(</span><span class="n">makeLearner</span><span class="p">(</span><span class="s2">"classif.randomForest"</span><span class="p">,</span><span class="w"> </span><span class="n">predict.type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"prob"</span><span class="p">),</span><span class="w">
                   </span><span class="n">sw.rate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rate</span><span class="p">,</span><span class="w"> </span><span class="n">sw.nn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">neighbors</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<h4 id="defining-the-resampling-strategy">Defining the Resampling Strategy</h4>

<p>Here we define the resampling technique. This strategy is implemented repeatedly throughout this notebook. Each time it chooses different records for each fold accounting for some of the variability between the models.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define the resampling technique</span><span class="w">
</span><span class="n">folds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="w">
</span><span class="n">rdesc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeResampleDesc</span><span class="p">(</span><span class="s2">"CV"</span><span class="p">,</span><span class="w"> </span><span class="n">iters</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">folds</span><span class="p">,</span><span class="w"> </span><span class="n">stratify</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="c1"># stratification with respect to the target</span><span class="w">
</span></code></pre></div></div>

<h2 id="benchmarking-logistic-regression">Benchmarking Logistic Regression</h2>

<p>First, we’ll consider the logistic regression and evaluate how SMOTE impacts model performance.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit the model</span><span class="w">
</span><span class="n">logreg_bchmk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">benchmark</span><span class="p">(</span><span class="n">logreg_lrns</span><span class="p">,</span><span class="w">
                  </span><span class="n">tsk_train_4wk</span><span class="p">,</span><span class="w">
                  </span><span class="n">rdesc</span><span class="p">,</span><span class="w"> </span><span class="n">show.info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
                  </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">acc</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">,</span><span class="w"> </span><span class="n">auc</span><span class="p">,</span><span class="w"> </span><span class="n">f1</span><span class="p">))</span><span class="w">
</span><span class="n">logreg_bchmk_perf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">getBMRAggrPerformances</span><span class="p">(</span><span class="n">logreg_bchmk</span><span class="p">,</span><span class="w"> </span><span class="n">as.df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">logreg_bchmk_perf</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">select</span><span class="p">(</span><span class="o">-</span><span class="n">task.id</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">knitr</span><span class="o">::</span><span class="n">kable</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left">learner.id</th>
      <th style="text-align: right">acc.test.mean</th>
      <th style="text-align: right">bac.test.mean</th>
      <th style="text-align: right">auc.test.mean</th>
      <th style="text-align: right">f1.test.mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">classif.logreg</td>
      <td style="text-align: right">0.9290151</td>
      <td style="text-align: right">0.4997191</td>
      <td style="text-align: right">0.8398939</td>
      <td style="text-align: right">0.0000000</td>
    </tr>
    <tr>
      <td style="text-align: left">classif.logreg.smoted</td>
      <td style="text-align: right">0.6743728</td>
      <td style="text-align: right">0.7880844</td>
      <td style="text-align: right">0.8234845</td>
      <td style="text-align: right">0.2869752</td>
    </tr>
  </tbody>
</table>

<p>Both models have nearly an identical AUC value of about 0.83. It seems these models effectively trading off accuracy and balanced accuracy. The SMOTE model has a higher balanced accuracy and F1 score of 78.8% and 0.29 respectively (compared to 50% and 0). And the vanilla model has a higher accuracy of 92.9% (compared to 67.4%).</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Visualize results</span><span class="w">
</span><span class="n">logreg_df_4wk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">generateThreshVsPerfData</span><span class="p">(</span><span class="n">logreg_bchmk</span><span class="p">,</span><span class="w">
            </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span><span class="w"> </span><span class="n">tpr</span><span class="p">,</span><span class="w"> </span><span class="n">mmce</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">,</span><span class="w"> </span><span class="n">ppv</span><span class="p">,</span><span class="w"> </span><span class="n">tnr</span><span class="p">,</span><span class="w"> </span><span class="n">fnr</span><span class="p">,</span><span class="w"> </span><span class="n">f1</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<h4 id="roc-curves">ROC Curves</h4>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotROCCurves</span><span class="p">(</span><span class="n">logreg_df_4wk</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/blog/images/VanillaVsSMOTE_files/unnamed-chunk-6-1.png" alt="" /></p>

<p>Looking at the ROC curves, we see that they intersect but otherwise have similar performance. It is important to note, in practice, we choose a threshold to operate a model. Therefore, the model with a larger area may not be the model with better performance within a limited threshold range.</p>

<h4 id="precision-recall-curves">Precision-Recall Curves</h4>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotROCCurves</span><span class="p">(</span><span class="n">logreg_df_4wk</span><span class="p">,</span><span class="w"> </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">tpr</span><span class="p">,</span><span class="w"> </span><span class="n">ppv</span><span class="p">),</span><span class="w"> </span><span class="n">diagonal</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/blog/images/VanillaVsSMOTE_files/unnamed-chunk-7-1.png" alt="" /></p>

<p>Here, if you are considering AUC-PR, the vanilla logistic regression does better than the SMOTEd model. Another thing to note is that the positive predictive value (precision) is fairly low for both models. Even though the AUC looked decent at 0.83 there is still a lot of imprecision in these models. Otherwise, the SMOTE model generally does better when recall (TPR) is high and vice verse for the vanilla model.</p>

<h4 id="threshold-plots">Threshold Plots</h4>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotThreshVsPerf</span><span class="p">(</span><span class="n">logreg_df_4wk</span><span class="p">,</span><span class="w">  </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span><span class="w"> </span><span class="n">fnr</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p><img src="/blog/images/VanillaVsSMOTE_files/unnamed-chunk-8-1.png" alt="" /></p>

<p>Threshold plots are common visualizations that help determine an appropriate threshold on which to operate. The FPR and FNR clearly illustrate the opposing tradeoff of each model. However it is difficult to compare these models using FPR and FNR since the imbalanced nature of the data has effectively squished the vanilla logistic model to the far left: slope is zero when the threshold is greater than ≈ 0.4.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotThreshVsPerf</span><span class="p">(</span><span class="n">logreg_df_4wk</span><span class="p">,</span><span class="w">  </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p><img src="/blog/images/VanillaVsSMOTE_files/unnamed-chunk-9-1.png" alt="" /></p>

<p>For our use case, threshold plots for F1 score and balanced accuracy make it easier to identify good thresholds. And while the vanilla logistic regression is still squished to the left, we can compare the performance peaks for the models. For F1, the vanilla model tends to have a higher peak. Whereas for balanced accuracy, SMOTE tends to have a slightly higher peak. Notice that the balanced accuracy for the SMOTEd model centers around the default threshold of 0.5 whereas the F1 score does not.</p>

<h3 id="confusion-matrices">Confusion Matrices</h3>

<p>To calculate the confusion matrices, we’ll train a new model using the full training set and predict against the holdout. Before, we only used the training data and aggregated the performance of the 20 cross-validated folds. We separate the data this way to prevent biasing our operating thresholds for these models.</p>

<p>The training set and holdout are defined at the beginning of this notebook and do not change. However, after tuning the SMOTE parameters, we rerun the cross-validation which may result in changes to the SMOTE model and operating thresholds for both models.</p>

<h4 id="vanilla-logistic-regression-default-threshold">Vanilla Logistic Regression (default threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left      0     67     67
  Stayed    0    884      0
  -err.-    0     67     67

      acc       bac       auc        f1
0.9295478 0.5000000 0.8041298 0.0000000
</code></pre></div></div>

<p>If you just look at accuracy (93%), this model performs great! But it is useless for the business. This model predicted that 0 employees would leave in the next 4 weeks but actually 67 left. This is why we need balanced performance measures like balanced accuracy for imbalanced classification problems. The balanced accuracy of 50% clearly illustrates the problem of this model.</p>

<h4 id="smote-logistic-regression-default-threshold">SMOTE Logistic Regression (default threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     56     11     11
  Stayed  277    607    277
  -err.-  277     11    288

      acc       bac       auc        f1
0.6971609 0.7612362 0.8177382 0.2800000
</code></pre></div></div>

<p>This is the first evidence that SMOTE works. We have a more balanced model (76.1% balanced accuracy compared to 50%) that might actually be useful for the business. It narrows the pool of employees at risk of attrition from 951 down to 333 while capturing 83.6% of employees that actually left. If this were the only information available, then SMOTE does appear to result in a better model.</p>

<p>However the AUC is similar for both models indicating similar performance. As mentioned earlier, we can operate these models at different thresholds.</p>

<h4 id="tuning-the-operating-threshold">Tuning the Operating Threshold</h4>

<p>The following code tunes the operating threshold for each model:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">metric</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">f1</span><span class="w">
</span><span class="n">logreg_thresh_vanilla</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tuneThreshold</span><span class="p">(</span><span class="w">
                             </span><span class="n">getBMRPredictions</span><span class="p">(</span><span class="n">logreg_bchmk</span><span class="w">
                                  </span><span class="p">,</span><span class="n">learner.ids</span><span class="w"> </span><span class="o">=</span><span class="s2">"classif.logreg"</span><span class="w">
                                  </span><span class="p">,</span><span class="n">drop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
                             </span><span class="p">,</span><span class="n">measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric</span><span class="p">)</span><span class="w">
</span><span class="n">logreg_thresh_SMOTE</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tuneThreshold</span><span class="p">(</span><span class="w">
                             </span><span class="n">getBMRPredictions</span><span class="p">(</span><span class="n">logreg_bchmk</span><span class="w">
                                  </span><span class="p">,</span><span class="n">learner.ids</span><span class="w"> </span><span class="o">=</span><span class="s2">"classif.logreg.smoted"</span><span class="w">
                                  </span><span class="p">,</span><span class="n">drop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
                             </span><span class="p">,</span><span class="n">measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>Here we’ve tuned these models using the F1 measure but we could have easily used a different metric.</p>

<h4 id="vanilla-logistic-regression-tuned-threshold">Vanilla Logistic Regression (tuned threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     29     38     38
  Stayed  130    754    130
  -err.-  130     38    168

      acc       bac       auc        f1
0.8233438 0.6428885 0.8041298 0.2566372
</code></pre></div></div>

<h4 id="smote-logistic-regression-tuned-threshold">SMOTE Logistic Regression (tuned threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     39     28     28
  Stayed  164    720    164
  -err.-  164     28    192

      acc       bac       auc        f1
0.7981073 0.6982846 0.8177382 0.2888889
</code></pre></div></div>

<p>Setting the tuned operating threshold results in two very similar models! Depending on the run, there might be a slight benefit to the SMOTEd model, but not enough to say with confidence.</p>

<p>But perhaps SMOTE just needs some tuning.</p>

<h4 id="tuning-smote">Tuning SMOTE</h4>

<p>The SMOTE algorithm is defined by the parameters <em>rate</em> and <em>nearest neighbors</em>. <em>Rate</em> defines how much to oversample the minority class. <em>Nearest neighbors</em> defines how many nearest neighbors to consider. Tuning these should result in better model performance.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">logreg_ps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeParamSet</span><span class="p">(</span><span class="w">
              </span><span class="n">makeIntegerParam</span><span class="p">(</span><span class="s2">"sw.rate"</span><span class="p">,</span><span class="w"> </span><span class="n">lower</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8L</span><span class="p">,</span><span class="w"> </span><span class="n">upper</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">28L</span><span class="p">)</span><span class="w">
              </span><span class="p">,</span><span class="n">makeIntegerParam</span><span class="p">(</span><span class="s2">"sw.nn"</span><span class="p">,</span><span class="w"> </span><span class="n">lower</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2L</span><span class="p">,</span><span class="w"> </span><span class="n">upper</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8L</span><span class="p">)</span><span class="w">
              </span><span class="p">)</span><span class="w">
</span><span class="n">ctrl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeTuneControlIrace</span><span class="p">(</span><span class="n">maxExperiments</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">400L</span><span class="p">)</span><span class="w">
</span><span class="n">logreg_tr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tuneParams</span><span class="p">(</span><span class="n">logreg_lrns</span><span class="p">[[</span><span class="m">2</span><span class="p">]],</span><span class="w"> </span><span class="n">tsk_train_4wk</span><span class="p">,</span><span class="w"> </span><span class="n">rdesc</span><span class="p">,</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">),</span><span class="w"> </span><span class="n">logreg_ps</span><span class="p">,</span><span class="w"> </span><span class="n">ctrl</span><span class="p">)</span><span class="w">
</span><span class="n">logreg_lrns</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">setHyperPars</span><span class="p">(</span><span class="n">logreg_lrns</span><span class="p">[[</span><span class="m">2</span><span class="p">]],</span><span class="w"> </span><span class="n">par.vals</span><span class="o">=</span><span class="n">logreg_tr</span><span class="o">$</span><span class="n">x</span><span class="p">)</span><span class="w">

</span><span class="c1"># Fit the model</span><span class="w">
</span><span class="n">logreg_bchmk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">benchmark</span><span class="p">(</span><span class="n">logreg_lrns</span><span class="p">,</span><span class="w">
                  </span><span class="n">tsk_train_4wk</span><span class="p">,</span><span class="w">
                  </span><span class="n">rdesc</span><span class="p">,</span><span class="w"> </span><span class="n">show.info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
                  </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">acc</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">,</span><span class="w"> </span><span class="n">auc</span><span class="p">,</span><span class="w"> </span><span class="n">f1</span><span class="p">))</span><span class="w">
</span><span class="n">logreg_thresh_vanilla</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tuneThreshold</span><span class="p">(</span><span class="w">
                                  </span><span class="n">getBMRPredictions</span><span class="p">(</span><span class="n">logreg_bchmk</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">learner.ids</span><span class="w"> </span><span class="o">=</span><span class="s2">"classif.logreg"</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">drop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
                                  </span><span class="n">measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric</span><span class="p">)</span><span class="w">
</span><span class="n">logreg_thresh_SMOTE</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tuneThreshold</span><span class="p">(</span><span class="w">
                                  </span><span class="n">getBMRPredictions</span><span class="p">(</span><span class="n">logreg_bchmk</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">learner.ids</span><span class="w"> </span><span class="o">=</span><span class="s2">"classif.logreg.smoted"</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">drop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
                                  </span><span class="n">measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h4 id="vanilla-logistic-regression-tuned-threshold-1">Vanilla Logistic Regression (tuned threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     35     32     32
  Stayed  145    739    145
  -err.-  145     32    177

      acc       bac       auc        f1
0.8138801 0.6791805 0.8041298 0.2834008
</code></pre></div></div>

<h4 id="smote-logistic-regression-tuned-threshold-and-smote">SMOTE Logistic Regression (tuned threshold and SMOTE)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     42     25     25
  Stayed  182    702    182
  -err.-  182     25    207

      acc       bac       auc        f1
0.7823344 0.7104917 0.8105795 0.2886598
</code></pre></div></div>

<p>If we account for resampling variance, the tuned SMOTE makes little difference. Perhaps the initial SMOTE parameters close enough to the optimal settings. This table shows how they changed:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: center">Rate</th>
      <th style="text-align: center">Nearest Neighbors</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">Initial</td>
      <td style="text-align: center">18</td>
      <td style="text-align: center">5</td>
    </tr>
    <tr>
      <td style="text-align: right">Tuned</td>
      <td style="text-align: center">9</td>
      <td style="text-align: center">2</td>
    </tr>
  </tbody>
</table>

<p>The rate decreased by 9 and the number of nearest neighbors decreased by 3.</p>

<p>After running this code multiple times, SMOTE generally produces models with higher balanced accuracy but lower accuracy. In terms of AUC and F1, it is harder to tell. Either way, even if SMOTE is tuned, observed performance increases are small compared to a vanilla logistic model with a tuned operating threshold. These results may also depend on the data itself. A different dataset intended to solve another imbalanced classification problem may have different results using SMOTE with logistic regression.</p>

<h2 id="benchmarking-decision-tree">Benchmarking Decision Tree</h2>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit the model</span><span class="w">
</span><span class="n">rpart_bchmk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">benchmark</span><span class="p">(</span><span class="n">rpart_lrns</span><span class="p">,</span><span class="w">
                  </span><span class="n">tsk_train_4wk</span><span class="p">,</span><span class="w">
                  </span><span class="n">rdesc</span><span class="p">,</span><span class="w"> </span><span class="n">show.info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
                  </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">acc</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">,</span><span class="w"> </span><span class="n">auc</span><span class="p">,</span><span class="w"> </span><span class="n">f1</span><span class="p">))</span><span class="w">
</span><span class="n">rpart_bchmk_perf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">getBMRAggrPerformances</span><span class="p">(</span><span class="n">rpart_bchmk</span><span class="p">,</span><span class="w"> </span><span class="n">as.df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">rpart_bchmk_perf</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">select</span><span class="p">(</span><span class="o">-</span><span class="n">task.id</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">knitr</span><span class="o">::</span><span class="n">kable</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left">learner.id</th>
      <th style="text-align: right">acc.test.mean</th>
      <th style="text-align: right">bac.test.mean</th>
      <th style="text-align: right">auc.test.mean</th>
      <th style="text-align: right">f1.test.mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">classif.rpart</td>
      <td style="text-align: right">0.9290539</td>
      <td style="text-align: right">0.5888060</td>
      <td style="text-align: right">0.6995163</td>
      <td style="text-align: right">0.2594619</td>
    </tr>
    <tr>
      <td style="text-align: left">classif.rpart.smoted</td>
      <td style="text-align: right">0.7491146</td>
      <td style="text-align: right">0.7771476</td>
      <td style="text-align: right">0.8605376</td>
      <td style="text-align: right">0.3113137</td>
    </tr>
  </tbody>
</table>

<p>Let’s be honest, the SMOTEd logistic regression was lackluster. But for the decision tree model, SMOTE increases AUC by 0.16. Both flavors have similar F1 scores; otherwise we see the same tradeoff between accuracy and balanced accuracy as in the logistic regression.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Visualize results</span><span class="w">
</span><span class="n">rpart_df_4wk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">generateThreshVsPerfData</span><span class="p">(</span><span class="n">rpart_bchmk</span><span class="p">,</span><span class="w">
            </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span><span class="w"> </span><span class="n">tpr</span><span class="p">,</span><span class="w"> </span><span class="n">mmce</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">,</span><span class="w"> </span><span class="n">ppv</span><span class="p">,</span><span class="w"> </span><span class="n">tnr</span><span class="p">,</span><span class="w"> </span><span class="n">fnr</span><span class="p">,</span><span class="w"> </span><span class="n">f1</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<h4 id="roc-curves-1">ROC Curves</h4>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotROCCurves</span><span class="p">(</span><span class="n">rpart_df_4wk</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/blog/images/VanillaVsSMOTE_files/unnamed-chunk-20-1.png" alt="" /></p>

<p>It’s easy to see that SMOTE has a higher AUC than the vanilla model, but since the lines cross, each perform better within certain operating thresholds.</p>

<h4 id="precision-recall-curves-1">Precision-Recall Curves</h4>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotROCCurves</span><span class="p">(</span><span class="n">rpart_df_4wk</span><span class="p">,</span><span class="w"> </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">tpr</span><span class="p">,</span><span class="w"> </span><span class="n">ppv</span><span class="p">),</span><span class="w"> </span><span class="n">diagonal</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/blog/images/VanillaVsSMOTE_files/unnamed-chunk-21-1.png" alt="" /></p>

<p>The vanilla model scores much higher on precision (PPV) but declines much more quickly as recall increases. SMOTE is more precise when recall (TPR) is greater than ≈ 0.75. Additionally, notice the straight lines, likely, there are no data in these regions making each model only viable for half the PR Curve.</p>

<h4 id="threshold-plots-1">Threshold Plots</h4>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotThreshVsPerf</span><span class="p">(</span><span class="n">rpart_df_4wk</span><span class="p">,</span><span class="w">  </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span><span class="w"> </span><span class="n">fnr</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p><img src="/blog/images/VanillaVsSMOTE_files/unnamed-chunk-22-1.png" alt="" /></p>

<p>The nearly vertical slopes of these threshold plots represent the straight lines on the PR Curve plot.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotThreshVsPerf</span><span class="p">(</span><span class="n">rpart_df_4wk</span><span class="p">,</span><span class="w">  </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p><img src="/blog/images/VanillaVsSMOTE_files/unnamed-chunk-23-1.png" alt="" /></p>

<p>If we’re concerned primarily with balanced accuracy, SMOTE is clearly better at all thresholds. For the F1 score however, it depends on the operating threshold of the model. Notice balanced accuracy is once again centered around the default threshold of 0.5 and the F1 measure is not. The F1 performance to threshold pattern is roughly opposite for the two flavors of decision trees.</p>

<h3 id="confusion-matrices-1">Confusion Matrices</h3>

<h4 id="vanilla-decision-tree-default-threshold">Vanilla Decision Tree (default threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     10     57     57
  Stayed    7    877      7
  -err.-    7     57     64

      acc       bac       auc        f1
0.9327024 0.5706676 0.7061694 0.2380952
</code></pre></div></div>

<p>Using the default threshold, the vanilla decision tree manages to identify some employee attrition. In face, its accuracy of 93.3% is higher than the baseline case of always predicting the majority class (93%). It is relatively precise (0.59) but has low recall (0.15). Overall accuracy is high (93.3%), but the model is not very balanced (57.1%).</p>

<h4 id="smote-decision-tree-default-threshold">SMOTE Decision Tree (default threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     55     12     12
  Stayed  232    652    232
  -err.-  232     12    244

      acc       bac       auc        f1
0.7434280 0.7792260 0.8338117 0.3107345
</code></pre></div></div>

<p>The SMOTE Decision Tree does a much better job of capturing employees that left (82% compared to 15%) but at the cost of precision. The model identifies 287 when only 55 from that group actually leave. Still this model is more useful to the business than the vanilla decision tree at the default threshold.</p>

<h4 id="tuning-the-operating-threshold-1">Tuning the Operating Threshold</h4>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rpart_thresh_vanilla</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tuneThreshold</span><span class="p">(</span><span class="w">
                                  </span><span class="n">getBMRPredictions</span><span class="p">(</span><span class="n">rpart_bchmk</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">learner.ids</span><span class="w"> </span><span class="o">=</span><span class="s2">"classif.rpart"</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">drop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
                                  </span><span class="n">measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric</span><span class="p">)</span><span class="w">
</span><span class="n">rpart_thresh_SMOTE</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tuneThreshold</span><span class="p">(</span><span class="w">
                                  </span><span class="n">getBMRPredictions</span><span class="p">(</span><span class="n">rpart_bchmk</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">learner.ids</span><span class="w"> </span><span class="o">=</span><span class="s2">"classif.rpart.smoted"</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">drop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
                                  </span><span class="n">measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>As before, we’ll be using the F1 measure.</p>

<h4 id="vanilla-decision-tree-tuned-threshold">Vanilla Decision Tree (tuned threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     22     45     45
  Stayed   26    858     26
  -err.-   26     45     71

      acc       bac       auc        f1
0.9253417 0.6494732 0.7061694 0.3826087
</code></pre></div></div>

<p>Once we tune the threshold, the vanilla decision tree model performs much better–identifying more employees that leave with relatively high precision. The F1 score increases from 0.238 to 0.383.</p>

<h4 id="smote-decision-tree-tuned-threshold">SMOTE Decision Tree (tuned threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     32     35     35
  Stayed   80    804     80
  -err.-   80     35    115

      acc       bac       auc        f1
0.8790747 0.6935571 0.8338117 0.3575419
</code></pre></div></div>

<p>Changing the operating threshold for the SMOTEd decision tree results in a 13.6% higher accuracy, 8.57% lower balanced accuracy, and higher F1 measure of 4.68%.</p>

<p>These changes to the operating threshold result in a similar F1 performance for both flavors of decision tree (0.358 compared to 0.383).</p>

<h4 id="tuning-smote-1">Tuning SMOTE</h4>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rpart_ps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeParamSet</span><span class="p">(</span><span class="w">
              </span><span class="n">makeIntegerParam</span><span class="p">(</span><span class="s2">"sw.rate"</span><span class="p">,</span><span class="w"> </span><span class="n">lower</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8L</span><span class="p">,</span><span class="w"> </span><span class="n">upper</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">28L</span><span class="p">)</span><span class="w">
              </span><span class="p">,</span><span class="n">makeIntegerParam</span><span class="p">(</span><span class="s2">"sw.nn"</span><span class="p">,</span><span class="w"> </span><span class="n">lower</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2L</span><span class="p">,</span><span class="w"> </span><span class="n">upper</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8L</span><span class="p">)</span><span class="w">
              </span><span class="p">)</span><span class="w">
</span><span class="n">ctrl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeTuneControlIrace</span><span class="p">(</span><span class="n">maxExperiments</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">200L</span><span class="p">)</span><span class="w">
</span><span class="n">rpart_tr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tuneParams</span><span class="p">(</span><span class="n">rpart_lrns</span><span class="p">[[</span><span class="m">2</span><span class="p">]],</span><span class="w"> </span><span class="n">tsk_train_4wk</span><span class="p">,</span><span class="w"> </span><span class="n">rdesc</span><span class="p">,</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">),</span><span class="w"> </span><span class="n">rpart_ps</span><span class="p">,</span><span class="w"> </span><span class="n">ctrl</span><span class="p">)</span><span class="w">
</span><span class="n">rpart_lrns</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">setHyperPars</span><span class="p">(</span><span class="n">rpart_lrns</span><span class="p">[[</span><span class="m">2</span><span class="p">]],</span><span class="w"> </span><span class="n">par.vals</span><span class="o">=</span><span class="n">rpart_tr</span><span class="o">$</span><span class="n">x</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit the model</span><span class="w">
</span><span class="n">rpart_bchmk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">benchmark</span><span class="p">(</span><span class="n">rpart_lrns</span><span class="p">,</span><span class="w">
                  </span><span class="n">tsk_train_4wk</span><span class="p">,</span><span class="w">
                  </span><span class="n">rdesc</span><span class="p">,</span><span class="w"> </span><span class="n">show.info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
                  </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">acc</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">,</span><span class="w"> </span><span class="n">auc</span><span class="p">,</span><span class="w"> </span><span class="n">f1</span><span class="p">))</span><span class="w">
</span><span class="n">rpart_thresh_vanilla</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tuneThreshold</span><span class="p">(</span><span class="w">
                                  </span><span class="n">getBMRPredictions</span><span class="p">(</span><span class="n">rpart_bchmk</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">learner.ids</span><span class="w"> </span><span class="o">=</span><span class="s2">"classif.rpart"</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">drop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
                                  </span><span class="n">measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric</span><span class="p">)</span><span class="w">
</span><span class="n">rpart_thresh_SMOTE</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tuneThreshold</span><span class="p">(</span><span class="w">
                                  </span><span class="n">getBMRPredictions</span><span class="p">(</span><span class="n">rpart_bchmk</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">learner.ids</span><span class="w"> </span><span class="o">=</span><span class="s2">"classif.rpart.smoted"</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">drop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
                                  </span><span class="n">measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h4 id="vanilla-decision-tree-tuned-threshold-1">Vanilla Decision Tree (tuned threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     23     44     44
  Stayed   35    849     35
  -err.-   35     44     79

      acc       bac       auc        f1
0.9169295 0.6518454 0.7061694 0.3680000
</code></pre></div></div>

<h4 id="smote-decision-tree-tuned-threshold-and-smote">SMOTE Decision Tree (tuned threshold and SMOTE)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     32     35     35
  Stayed   80    804     80
  -err.-   80     35    115

      acc       bac       auc        f1
0.8790747 0.6935571 0.7885628 0.3575419
</code></pre></div></div>

<p>Tuning SMOTE for the decision tree changed the accuracy from 87.9% to 87.9% and the balanced accuracy from 69.4% to 69.4%. The following table shows how the rate and number of nearest neighbors changed:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: center">Rate</th>
      <th style="text-align: center">Nearest Neighbors</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">Initial</td>
      <td style="text-align: center">18</td>
      <td style="text-align: center">5</td>
    </tr>
    <tr>
      <td style="text-align: right">Tuned</td>
      <td style="text-align: center">15</td>
      <td style="text-align: center">7</td>
    </tr>
  </tbody>
</table>

<p>The rate decreased by 3 and the number of nearest neighbors increased by 2.</p>

<p>Given our data, SMOTE for decision trees seems to offer real performance increases to the model. That said, the performance increases are largely via tradeoff between accuracy and balanced accuracy. Setting the operating threshold for the vanilla model results in a similarly performant model. However, we need to consider that identifying rare events is our primary concern. SMOTE allows us to operate with increased performance when high recall is important.</p>

<h2 id="benchmarking-randomforest">Benchmarking randomForest</h2>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit the model</span><span class="w">
</span><span class="n">randomForest_bchmk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">benchmark</span><span class="p">(</span><span class="n">randomForest_lrns</span><span class="p">,</span><span class="w">
                  </span><span class="n">tsk_train_4wk</span><span class="p">,</span><span class="w">
                  </span><span class="n">rdesc</span><span class="p">,</span><span class="w"> </span><span class="n">show.info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
                  </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">acc</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">,</span><span class="w"> </span><span class="n">auc</span><span class="p">,</span><span class="w"> </span><span class="n">f1</span><span class="p">))</span><span class="w">
</span><span class="n">randomForest_bchmk_perf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">getBMRAggrPerformances</span><span class="p">(</span><span class="n">randomForest_bchmk</span><span class="p">,</span><span class="w"> </span><span class="n">as.df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="n">randomForest_bchmk_perf</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">select</span><span class="p">(</span><span class="o">-</span><span class="n">task.id</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">knitr</span><span class="o">::</span><span class="n">kable</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<table>
  <thead>
    <tr>
      <th style="text-align: left">learner.id</th>
      <th style="text-align: right">acc.test.mean</th>
      <th style="text-align: right">bac.test.mean</th>
      <th style="text-align: right">auc.test.mean</th>
      <th style="text-align: right">f1.test.mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">classif.randomForest</td>
      <td style="text-align: right">0.9310609</td>
      <td style="text-align: right">0.5878136</td>
      <td style="text-align: right">0.8942103</td>
      <td style="text-align: right">0.2674242</td>
    </tr>
    <tr>
      <td style="text-align: left">classif.randomForest.smoted</td>
      <td style="text-align: right">0.8868540</td>
      <td style="text-align: right">0.7553178</td>
      <td style="text-align: right">0.8875052</td>
      <td style="text-align: right">0.4339340</td>
    </tr>
  </tbody>
</table>

<p>For the randomForest models, we see similar patters as for the logistic regression and decision tress. There is a trade off between accuracy and balanced accuracy. Unlike the decision tree models, SMOTE does not improve AUC for SMOTE randomForest (both are ≈ 0.89).</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Visualize results</span><span class="w">
</span><span class="n">randomForest_df_4wk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">generateThreshVsPerfData</span><span class="p">(</span><span class="n">randomForest_bchmk</span><span class="p">,</span><span class="w">
            </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span><span class="w"> </span><span class="n">tpr</span><span class="p">,</span><span class="w"> </span><span class="n">mmce</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">,</span><span class="w"> </span><span class="n">ppv</span><span class="p">,</span><span class="w"> </span><span class="n">tnr</span><span class="p">,</span><span class="w"> </span><span class="n">fnr</span><span class="p">,</span><span class="w"> </span><span class="n">f1</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<h4 id="roc-curves-2">ROC Curves</h4>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotROCCurves</span><span class="p">(</span><span class="n">randomForest_df_4wk</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/blog/images/VanillaVsSMOTE_files/unnamed-chunk-35-1.png" alt="" /></p>

<p>Both models cross multiple times showing either model is likely good for most thresholds.</p>

<h4 id="precision-recall-curves-2">Precision-Recall Curves</h4>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotROCCurves</span><span class="p">(</span><span class="n">randomForest_df_4wk</span><span class="p">,</span><span class="w"> </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">tpr</span><span class="p">,</span><span class="w"> </span><span class="n">ppv</span><span class="p">),</span><span class="w"> </span><span class="n">diagonal</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="/blog/images/VanillaVsSMOTE_files/unnamed-chunk-36-1.png" alt="" /></p>

<p>This PR-Curve shows more distinctly that SMOTE generally performs better when recall is high, whereas the vanilla model generally performs better when recall is lower.</p>

<h4 id="threshold-plots-2">Threshold Plots</h4>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotThreshVsPerf</span><span class="p">(</span><span class="n">randomForest_df_4wk</span><span class="p">,</span><span class="w">  </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span><span class="w"> </span><span class="n">fnr</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p><img src="/blog/images/VanillaVsSMOTE_files/unnamed-chunk-37-1.png" alt="" /></p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plotThreshVsPerf</span><span class="p">(</span><span class="n">randomForest_df_4wk</span><span class="p">,</span><span class="w">  </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<p><img src="/blog/images/VanillaVsSMOTE_files/unnamed-chunk-38-1.png" alt="" /></p>

<p>Interestingly, the SMOTE randomForest does not center balanced accuracy around the default threshold; rather F1 is centered on the 0.5 threshold. Otherwise we see that SMOTE produces a higher peak for balanced accuracy but lower for F1. Additionally, the vanilla model is still squished to the left due to its class imbalance.</p>

<h3 id="confusion-matrices-2">Confusion Matrices</h3>

<h4 id="vanilla-randomforest-default-threshold">Vanilla randomForest (default threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     15     52     52
  Stayed    7    877      7
  -err.-    7     52     59

      acc       bac       auc        f1
0.9379600 0.6079810 0.8559718 0.3370787
</code></pre></div></div>

<p>As far as vanilla models go, and given the default threshold, the randomForest performs the best. That said, this model captures very few employees that leave.</p>

<h4 id="smote-randomforest-default-threshold">SMOTE randomForest (default threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     34     33     33
  Stayed   78    806     78
  -err.-   78     33    111

      acc       bac       auc        f1
0.8832808 0.7096137 0.8686263 0.3798883
</code></pre></div></div>

<p>The SMOTEd randomForest also does well. The accuracy is high and manages a good balanced accuracy.</p>

<h4 id="tuning-the-operating-threshold-2">Tuning the Operating Threshold</h4>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">randomForest_thresh_vanilla</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tuneThreshold</span><span class="p">(</span><span class="w">
                                  </span><span class="n">getBMRPredictions</span><span class="p">(</span><span class="n">randomForest_bchmk</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">learner.ids</span><span class="w"> </span><span class="o">=</span><span class="s2">"classif.randomForest"</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">drop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
                                  </span><span class="n">measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric</span><span class="p">)</span><span class="w">
</span><span class="n">randomForest_thresh_SMOTE</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tuneThreshold</span><span class="p">(</span><span class="w">
                                  </span><span class="n">getBMRPredictions</span><span class="p">(</span><span class="n">randomForest_bchmk</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">learner.ids</span><span class="w"> </span><span class="o">=</span><span class="s2">"classif.randomForest.smoted"</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">drop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
                                  </span><span class="n">measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>As before, we’ll be using the F1 measure.</p>

<h4 id="vanilla-randomforest-tuned-threshold">Vanilla randomForest (tuned threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     38     29     29
  Stayed   78    806     78
  -err.-   78     29    107

      acc       bac       auc        f1
0.8874869 0.7394644 0.8559718 0.4153005
</code></pre></div></div>

<h4 id="smote-randomforest-tuned-threshold">SMOTE randomForest (tuned threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     36     31     31
  Stayed   81    803     81
  -err.-   81     31    112

      acc       bac       auc        f1
0.8822292 0.7228422 0.8686263 0.3913043
</code></pre></div></div>

<p>At the tuned threshold, the performance of both flavors perform better and are once again very similar. Depending on the run, SMOTE will have a higher balanced accuracy, but otherwise there is little difference between the models.</p>

<h4 id="tuning-smote-2">Tuning SMOTE</h4>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">randomForest_ps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeParamSet</span><span class="p">(</span><span class="w">
              </span><span class="n">makeIntegerParam</span><span class="p">(</span><span class="s2">"sw.rate"</span><span class="p">,</span><span class="w"> </span><span class="n">lower</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8L</span><span class="p">,</span><span class="w"> </span><span class="n">upper</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">28L</span><span class="p">)</span><span class="w">
              </span><span class="p">,</span><span class="n">makeIntegerParam</span><span class="p">(</span><span class="s2">"sw.nn"</span><span class="p">,</span><span class="w"> </span><span class="n">lower</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2L</span><span class="p">,</span><span class="w"> </span><span class="n">upper</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8L</span><span class="p">)</span><span class="w">
              </span><span class="p">)</span><span class="w">
</span><span class="n">ctrl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeTuneControlIrace</span><span class="p">(</span><span class="n">maxExperiments</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">200L</span><span class="p">)</span><span class="w">
</span><span class="n">randomForest_tr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tuneParams</span><span class="p">(</span><span class="n">randomForest_lrns</span><span class="p">[[</span><span class="m">2</span><span class="p">]],</span><span class="w"> </span><span class="n">tsk_train_4wk</span><span class="p">,</span><span class="w"> </span><span class="n">rdesc</span><span class="p">,</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">),</span><span class="w"> </span><span class="n">randomForest_ps</span><span class="p">,</span><span class="w"> </span><span class="n">ctrl</span><span class="p">)</span><span class="w">
</span><span class="n">randomForest_lrns</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">setHyperPars</span><span class="p">(</span><span class="n">randomForest_lrns</span><span class="p">[[</span><span class="m">2</span><span class="p">]],</span><span class="w"> </span><span class="n">par.vals</span><span class="o">=</span><span class="n">randomForest_tr</span><span class="o">$</span><span class="n">x</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit the model</span><span class="w">
</span><span class="n">randomForest_bchmk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">benchmark</span><span class="p">(</span><span class="n">randomForest_lrns</span><span class="p">,</span><span class="w">
                  </span><span class="n">tsk_train_4wk</span><span class="p">,</span><span class="w">
                  </span><span class="n">rdesc</span><span class="p">,</span><span class="w"> </span><span class="n">show.info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w">
                  </span><span class="n">measures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">acc</span><span class="p">,</span><span class="w"> </span><span class="n">bac</span><span class="p">,</span><span class="w"> </span><span class="n">auc</span><span class="p">,</span><span class="w"> </span><span class="n">f1</span><span class="p">))</span><span class="w">
</span><span class="n">randomForest_thresh_vanilla</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tuneThreshold</span><span class="p">(</span><span class="w">
                                  </span><span class="n">getBMRPredictions</span><span class="p">(</span><span class="n">randomForest_bchmk</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">learner.ids</span><span class="w"> </span><span class="o">=</span><span class="s2">"classif.randomForest"</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">drop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
                                  </span><span class="n">measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric</span><span class="p">)</span><span class="w">
</span><span class="n">randomForest_thresh_SMOTE</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tuneThreshold</span><span class="p">(</span><span class="w">
                                  </span><span class="n">getBMRPredictions</span><span class="p">(</span><span class="n">randomForest_bchmk</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">learner.ids</span><span class="w"> </span><span class="o">=</span><span class="s2">"classif.randomForest.smoted"</span><span class="w">
                                                    </span><span class="p">,</span><span class="n">drop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
                                  </span><span class="n">measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h4 id="vanilla-randomforest-tuned-threshold-1">Vanilla randomForest (tuned threshold)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     40     27     27
  Stayed  101    783    101
  -err.-  101     27    128

      acc       bac       auc        f1
0.8654048 0.7413808 0.8527470 0.3846154
</code></pre></div></div>

<h4 id="smote-randomforest-tuned-threshold-and-smote">SMOTE randomForest (tuned threshold and SMOTE)</h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        predicted
true     Left Stayed -err.-
  Left     37     30     30
  Stayed   98    786     98
  -err.-   98     30    128

      acc       bac       auc        f1
0.8654048 0.7206895 0.8637384 0.3663366
</code></pre></div></div>

<p>Given this data, tuning SMOTE does not seem to improve performance. The following table shows how the parameters for SMOTE changed during the tuning process:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: center">Rate</th>
      <th style="text-align: center">Nearest Neighbors</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">Initial</td>
      <td style="text-align: center">18</td>
      <td style="text-align: center">5</td>
    </tr>
    <tr>
      <td style="text-align: right">Tuned</td>
      <td style="text-align: center">18</td>
      <td style="text-align: center">3</td>
    </tr>
  </tbody>
</table>

<p>The rate did not change and the number of nearest neighbors decreased by 2.</p>

<p>Given this data for randomForest, SMOTE does little to improve model performance. At optimized operating thresholds, both flavors end up with very similar accuracy and balanced accuracy. There does appear to be some benefit using SMOTE where recall is high and precision is low, however the business may not want to throw such a large net in order to capture all of the employees that leave. Practically speaking, SMOTE did not improve the performance for this problem when using randomForest.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">parallelStop</span><span class="p">()</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Stopped parallelization. All cleaned up.
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>Given this data, SMOTE improved AUC of the decision tree model but offered little improvement for logistic regression or randomForest. Otherwise, SMOTE offered a way to trade accuracy for balanced accuracy. For our problem of employee attrition, this trade off is worth it to continue using SMOTE. Even when operating thresholds are optimized, there is–at worst–no change in the performance of the models. That said, the ideal solution might be an ensemble of SMOTE and vanilla models at operating thresholds suited for their flavor.</p>

<p>This notebook shows SMOTE impacts models differently, a finding supported by <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.4356&amp;rep=rep1&amp;type=pdf">Experimental Perspectives on Learning from Imbalanced Data</a>. They also found, while generally beneficial, SMOTE often did not perform as well as simple random undersampling–something we might try in a future notebook. A different paper, <a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-106">SMOTE for high-dimensional class-imbalanced data</a>, found that for high-dimensional data, SMOTE is beneficial but only after variable selection is performed. The employee attrition problem featured here does not have high-dimensional data, however it is useful to consider how feature selection may impact the calculated Euclidean distance used in the SMOTE algorithm. If we gather more features, it may be beneficial to perform more rigorous feature selection before SMOTE to improve model performance.</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="patdmob/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/tutorial/imbalanced%20classification/r%20programming/2018/05/16/Vanilla-vs-SMOTE.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/patdmob" title="patdmob"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/patrickdmobley" title="patrickdmobley"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/patdmob" title="patdmob"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
