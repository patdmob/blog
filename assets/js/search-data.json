{
  
    
        "post0": {
            "title": "Taking a .bat to scripts",
            "content": ". Let’s suppose that we need to periodically run the same scripts (written in R or Python perhaps), and furthermore we’re using Windows. One of many possible solutions to this problem is to use a .bat file and then use a task scheduler to periodically run those scripts. . But why use a .bat file? If you are automating scripts, then just run the script! In Windows, a .bat file has the advantage of being flexibly executed. I can double click on it, use the cmd prompt, or I can throw it into a task scheduler and it will run. This flexibility is especially useful if you inconsistently execute the script or have not-so-tech-savvy end users. . But with this flexibility does come a challenge. How do .bat files pass absolute/relative paths to internal script calls? To solve this problem, I developed a quick test case to experiment. I wrote my test case in R but could have just as easily written it in Python. . Test case . My project directory contains the .bat file and separate folders for data and the R script. . The directory looks like this: . . In the R script, I reference a file in the data folder. . test_data &lt;- readRDS(&quot;./data/test_data.rds&quot;) print(getwd()) print(test_data) fileConn &lt;- file(&quot;result.txt&quot;) writeLines(getwd(), fileConn) close(fileConn) . This code will use a relative path to retrieve data from a separate folder and expects the root path “./” to be the project directory. . I’m not an expert at writing .bat files so I wrote a couple different scenarios to see what would happen. Eventually, I came up with the following code with comments. To the batch file (.bat) experts out there, please let me know if there is a better way. . @echo off REM This is a test to determine the working path REM when running RScript in BAT REM %CD% gives the path the script was called from REM %~dp0 will give the path of the script itself echo Called path: echo %CD% echo. echo Script path: echo %~dp0 echo. pause echo Works only if you run it from the .bat&#39;s directory: Rscript &quot;%~dp0 R bat_test.R&quot; echo. echo Works even if you run it from another directory: cd %~dp0 Rscript &quot;R bat_test.R&quot; echo. echo Press enter to close . . . pause &gt;nul exit . Here were my results: . . As you can see, I called the .bat file from my user profile directory. The first attempt fails but the second is a success. By using this second method, a user can flexibly execute the .bat file by double clicking, navigating from the cmd prompt, or using the task scheduler. .",
            "url": "https://patdmob.github.io/blog/blog/bat%20files/automation/windows/2018/12/15/Taking-a-bat-to-scripts.html",
            "relUrl": "/blog/bat%20files/automation/windows/2018/12/15/Taking-a-bat-to-scripts.html",
            "date": " • Dec 15, 2018"
        }
        
    
  
    
        ,"post1": {
            "title": "Missing Data Modeled",
            "content": "Outline . Introduction Example data Understanding the problem &amp; defining success Investigating missingness Methods to deal with missing data Missing data packages in R Conclusion Resources . Introduction . Missing data analysis is often overlooked in the data modeling process, yet it is essential for developing high quality models. Before cleaning, nearly every dataset contains missing data. As we prepare the dataset for modeling, we make various assumptions that impact how the production model will interact in the real-world. . Unfortunately, we can’t leave these decisions to algorithms. Most learners simply drop any records with missing data. And models that “support” missing data often only perform basic mean/mode imputation. These common but naive approaches to missingness can erode the data leaving it underpowered and/or biased–unable to answer intended questions. Fortunately, missing data analysis has provided us with a framework to overcome these challenges. . In this notebook, I provide an overview to missing data analysis. After reading this, you should have a solid understanding of the problem, considerations, and techniques relevant for missing data analysis. . Example data . Admittedly, the iris dataset is overused. But the data isn’t the point. It’s just a vehicle to get where we’re going. In case you haven’t interacted with the Iris dataset before: . Iris Dataset . There are three types of iris flowers, each with measurements for sepal length, sepal width, petal length, and petal width. . Setosa | Versicolour | Virginica | . The dataset contains 50 observations for each flower. . Understanding the problem &amp; defining success . Depending on your experience, you may already understand why missing data is a problem. However, let me formally define the problem and state a few goals which define success. . Loss of power . Statistical power refers to the amount of data required to reliably make a statistical claim. Simply said, missing data results in less usable data. Occasionally we have data to spare, but even small amounts of missing data can drastically reduce the size of usable data. In many fields, data collection is extremely costly and sometimes cannot be replicated. . If we randomly drop 6% of values, then a sample of the Iris dataset might look like this: . Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 NA 0.2 setosa 3 NA 3.2 1.3 0.2 &lt;NA&gt; 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa . Most models require complete data (no missing values) from which to learn. After removing rows with missing data, we only have 113 out of 150 complete rows. Therefore, even small amounts of missing data (6%) can result in large data loss (24.7%) from a modeling perspective. Generally, a good solution to missing data will maximize available data. . Bias . The second way missing data impacts a dataset is through bias. Consider the following chart. In this case, random elements were dropped from the data if petal length was less than 3.0. Perhaps the data collector had an unconscious bias against short petal iris flowers. . . At first glance, the available case distribution looks similar to the real data. But a common mistake in exploratory data analysis is to consider each variable individually and not compare this to what the model actually uses. If there is a pattern to the missingness, complete-case (only using complete rows) modeling will result in a biased model. At this point we need to ask ourselves why the data is missing. There can be multiple reasons which may require different strategies to address. . If we can successfully rule out any patterns to the missingness, then we can assume it is Missing Completely at Random (MCAR). The following chart shows data that is MCAR. . . Notice, even though the sample size decreases, the general distribution remains the same. If the data is MCAR, then bias is not a concern; however data is rarely MCAR. Regardless, a good solution for missing data will seek to make bias as small as possible. . Estimating uncertainty . Any method we use will change the shape of the data. Ideally, this altered shape should resemble the real data as closely as possible. But exactly how close is it? Much of statistics is primarily concerned about two things: estimating the magnitude of a statistic (mean, median, variance, etc.) and the probability/likelihood of occurrence. If we replace missing values with an estimate, how do we account for the variability in that estimate? Said differently, how confident are we that our estimates are accurate? . For this reason, advanced techniques like Multiple Imputations using Chained Equations (MICE) give a distribution of potential values. This provides an excellent way to correct estimates of uncertainty. However, it also makes modeling more complicated and is not appropriate for all applications. Even so, we should strive for accurate estimates of uncertainty (standard errors, confidence intervals, p-values, etc.). . Goals for Missing Data Analysis . To summarize, our goals1 are to: . Maximize the use of available information. | Minimize bias. | Yield good estimates of uncertainty. | These goals define success for our missing data problem. Of course, we must also balance these goals against any technical constraints of our project. For instance, streaming analytics will need computationally efficient methods which might not ideally solve the missing data but still provide a good enough proximity for the application. If this happens, it’s important to document these model limitations. . Investigating missingness . It’s important to understand why the data is missing. This could mean one or even multiple reasons. This will help us to choose an appropriate method and avoid biasing our dataset. . Consider how the data was collected . By understanding more about the data collection process, you’re able to discover potential reasons why data might be missing. . For instance, survey data commonly has missing data. Questions might be confusing or entice people to lie. A good solution for this is to interview a subset of respondents and understand their thought process. Perhaps this subset just didn’t know how to interpret the question. . Depending on the environment, there might be regular events which impact the data. Monthly power generator tests may briefly shut down computers collecting data. By talking with building management, you can get a schedule of events which may impact your data. . Regardless of the cause, understanding how the data was collected, its consistency, environment, etc. all help you to understand potential sources bias and missing data. . Visualizing the problem . It is common in Exploratory Data Analysis (EDA) to only visualize what is there, forgetting about what is not. Actually, much of missing data analysis is part of EDA. But how do you visualize what is not there? You look at the pattern! . Here I’ve used a packaged called VIM to visualize the missingness pattern: . #install.packages(&quot;VIM&quot;) require(VIM) aggr(iris_mis, col=c(&#39;darkred&#39;,&#39;slategrey&#39;),numbers=TRUE,sortVars=TRUE, labels=names(iris_mis2),cex.axis=.8,gap=3,ylab=c(&quot;Proportion of Missing Data&quot;,&quot;Pattern&quot;)) . . On the left, we have a bar chart showing the proportion of missing data for each variable. And on the right, we have can see the pattern of missingness for each combination of variables. For instance, 75.3% of the data is complete with no missing values. . When visualizing missing data, think about why it might be missing missing and how you plan on using the data in a model. Is a variable with high missingness necessary for the model? Will dropping observations with high missingness induce bias or critically impact power? What is the most/least common missingness pattern? Are there any missingness dependencies between variables (e.g. are two variables are always missing together)? . You can also look at any correlations between missing and present data. If we looked at a data set of heights and weights, perhaps taller individuals were more likely to have missing weight. If so, visualizing the histogram of height by missing weight frequency may reveal an interesting pattern. . Three types of missing data . While researching the data collection process and performing EDA, it helps to understand the three types of missing data. Each type has different implications. To illustrate, imagine that you’re a field investigator collecting household surveys to understand the labor market. . MCAR - Missing Completely At Random . You have just finished collecting surveys. On your way back to the office, a stack of surveys take flight on a gust of wind. As you scramble to catch them, some of the responses are obscured. . This is a case of Missing Completely At Random (MCAR). We know this because an unrelated outside event resulted in missing data. In other words, the cause of missingness is unrelated to the data. The wind doesn’t care which answers are obscured. It was a completely random accident. . If your data is MCAR, great! It’s the easiest of all missingness to handle. MCAR is un-biased missing data. Simple statistic imputation (mean, median, mode, etc.) is a perfectly legitimate method when your data is MCAR. If you have enough data, deleting rows with missingness (complete-case analysis) is also an option. Unfortunately, MCAR is the least common form of missing data. . MAR - Missing At Random . After entering the data and exploratory data analysis, you realize that an occupational subgroup has a high rate of missingness concerning their income (e.g. perhaps attorneys prefer not to disclose their income). While not ideal, this missingness is manageable since you are able to use other variables to tease out their likely income. In other words, the missingness is biased in some way explainable by the data. . This type of missingness is considered to be Missing At Random (MAR). Ironically, this is not the best name since it is really conditionally missing at random. But it’s historical so it stuck. . MNAR - Missing Not At Random . Lastly, suppose larger households omit the number of people living in the household. In this case, the reason for missingness depends on the missing values themselves and not on other variables for which we have information. Another case of MNAR would be if an unmeasured 3rd party is trying to influence the data. . This kind of missingness is considered Missing Not At Random (MNAR). MNAR is bias unexplained by current data. Unfortunately this kind of missingness is difficult to test for or solve. Sometimes the best way to overcome MNAR is by collecting additional information. . Difference between MAR and MNAR . One way to think about the difference between MAR and MNAR missingness is based on available information. Let’s say that we have a dataset with only one variable: Income. If income is missing for given observations, we have no other information from which to make an educated guess. This is MNAR. However, if we collect Occupation, Employed, Education, Age, Experience, etc. we might be able to make a much better guess at income. Now the missing data is MAR. The key difference is whether the bias can be explained by the current data. . Methods to deal with missing data . Consider . Three things to consider when choosing a method for handling missing data: . What is the type and cause of missingness? MCAR, MAR, or MNAR | Likely cause(s) | . | What is the shape of the data? Ordinal, nominal, continuous, discrete, time series, panel etc. | What does the distribution look like? | . | What are my constraints? Sample size (too large, too small) | Streaming or batch | Training time | . | . Each method makes different assumptions or is optimized for a particular type of data. Choose the method that fits your data type and problem best. You will likely need to learn a few different methods to handle various types of data. More complicated algorithms like Multiple Imputation using Chained Equations (MICE) or Maximum Likelihood can be complicated to set up and may take too much computation for a given application. . Methods . I’ve listed methods below in order of least to most complicated. For each variable with missingness, consider the methods at the top first. However, if you are conserned about power and bias, then you’ll need to adopt more complicated methods. Finally, if you have many variables with missingness, you may need to use a specialized algorithm which performs multiple imputation. . Note: Here I make the distinction between replacement, interpolation, and imputation, however they are all forms of imputation (i.e replacing a missing value with an estimated value). . Deletion List-wise (complete-case analysis) | Pair-wise (available-case analysis) | Observations or variables with too little information to be useful | . | Replacement Static value: mean, median, mode, constant, zeros, “missing” for categorical variables | Dynamic value: logical rules, LVCF, hot-deck, cold-deck, random | . | Interpolation Appropriate for data following a predictable pattern (1, 2, ?, 4, 5…) | Common for time-series or spatial data | . | Missingness Indicator Indicator variable to denote a replaced/interpolated/imputed missing value. This assumes there is a unobserved reason/pattern for missingness, and if not can induce bias | . | Imputation Single imputation uses other features to predict/approximate the correct value of one variable. In reality, this can be any common model used by predicting the missing values (e.g. regression, knn, etc.). | Multiple imputation imputes multiple variables at the same time. The rational being if multiple variables have biased missingness, then imputing one variable on others would result in biased imputation. These are usually specialized missing data models which iterate over each variable with missingness until convergence (e.g. MICE, Maximum Likelihood, missForest, etc.). | . | Combination | . Occationally, it makes sense to use create a missingness indicator for select variables. Let’s suppose that variable x’s missingness likely represents a group uncaptured by other variables. Then we should create a missingness indicator for variable x and impute missing values for x. . A dataset will probably require multiple methods. It is common to drop a variable with lots of missingness, then interpolate a date field, before performing multiple imputation on the rest. . Keep in Mind . All models are wrong, but some are useful . — famed statistician George Box . Your solution to missing data is just a model. It’s wrong, but it can be useful. Don’t take it too seriously, but know that it is important. . Missing data packages in R . This is by no means an exhaustive list, but these packages are fairly popular. . Imputation Hmisc - General package with functions for single and multiple imputation | missForest - Muliple Imputation based on Random Forest algorithm | MICE - Muliple Imputation using Chained Equations | Amelia - Imputation for time series data | . | Tools and visualization mitools - Tools for multiple imputation of missing data | VIM - Visualization and Imputation of Missing values | . | . In the following examples, we’ll use the biased Iris dataset (random elements were dropped if petal length was less than 3.0). . Example in Hmisc . #install.packages(&quot;Hmisc&quot;) require(Hmisc) . impute() function replaces the missing value with user defined statistical method (default=median | mean | max etc.) | . iris_mis$imputed.Petal.Length &lt;- with(iris_mis, impute(Petal.Length, mean)) iris_mis$imputed.Petal.Width &lt;- with(iris_mis, impute(Petal.Width, mean)) . However, as we can see below, using mean imputation increases the bias making it difficult to identify Setosa, the species with the smallest petal length. . . For this reason, you should use more advanced methods when dealing with biased missing data. Even unbiased data will benefit depending on your choice of analytic model (decision trees are probably more affected by mean imputation than linear regression). . aregImpute() function creates multiple imputations using additive regression, bootstrapping, and predictive mean matching. . imputed_aregImpute &lt;- aregImpute(~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width + Species, data = iris_mis, n.impute = 5) . 5 separate imputations to yield good estimates of uncertainty. The literature suggests 20 or more imputations. . But for now, we want to pool those imputations to graph the results. . imputed &lt;- as.data.frame(impute.transcan(imputed_aregImpute, imputation = 1, data = iris_mis, list.out = TRUE, pr=FALSE, check=FALSE)) . Here, muliple imputation does a much better job at matching the shape of the original data. . . Example in MICE . MICE stands for Multivariate Imputation using Chained Equations . #install.packages(&quot;mice&quot;) require(mice) imputed_mice &lt;- mice(data = iris_mis[1:5], m = 5, method = &quot;pmm&quot;, maxit=50, seed=500) . It is one of the more complicated methods, however it does a great job of imputing missing variables. . . Example in missForest . #install.packages(&quot;missForest&quot;) require(missForest) . missForest is a relatively newer imputation algorithm and uses an iterative random forest approach to missing data. It accomplishes this by estimating the accuracy of these predictions and adjusts the model. You can also run it in parallel when installing the package doParallel. . Imputing with missForest: . #install.packages(&quot;doParallel&quot;) require(doParallel) registerDoParallel() #registering the processor getDoParWorkers() #number of processors running #vignette(&quot;gettingstartedParallel&quot;) #for more information imputed_forest &lt;- missForest(iris_mis[1:5], parallelize = &quot;forest&quot;) imputed_forest$OOBerror #calling Out Of Bag error iris_mis.forest &lt;- imputed_forest$ximp . As you can see, it also does quite well. . . Conclusion . While often overlooked, missing data analysis plays an important role in the modeling process. Common but naive approaches often leave datasets underpowered or biased which could impact real-world model performance. . Missing data analysis is conscerned with making the best use of availible information while minimizing bias and maintaining accurate estimates of uncertainty. To do this, it is important to understand the type of and causes for missingness. We also need to balance the goals of success with the technical constraints of the problem. Afterward, a reasonable approach to handle missing data often becomes clear. . There are a variety of methods developed to overcome the problem of missing data. This article highlights a few of them. As this is a growing field, new methods constantly appear. . Resources . This notebook: . https://github.com/patdmob/Missing-Data . Articles: . http://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/ . http://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/ . https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/ . Chapters: . http://www.statisticalhorizons.com/wp-content/uploads/2012/01/Milsap-Allison.pdf . Footnotes . 1 These goals are adopted from Paul Allison’s chapter on missing data referenced in the resources section. ↩ .",
            "url": "https://patdmob.github.io/blog/tutorial/r%20programming/missing%20data/2018/07/15/Missing-Data-Modeled.html",
            "relUrl": "/tutorial/r%20programming/missing%20data/2018/07/15/Missing-Data-Modeled.html",
            "date": " • Jul 15, 2018"
        }
        
    
  
    
        ,"post2": {
            "title": "Vanilla vs SMOTE Flavored Imbalanced Classification",
            "content": "Outline . Introduction | Setup | Benchmarking Logistic Regression | Benchmarking Decision Tree | Benchmarking randomForest | Conclusion | . Introduction . This is a companion notebook to Imbalanced Classification with mlr. In this notebook, we investigate whether SMOTE actually improves model performance. For clarity, non-SMOTE models are referred to as “vanilla” models. We compare these two flavors (vanilla and SMOTE) using logistic regression, decision trees, and randomForest. We also consider how tuning model operating thresholds and tuning SMOTE parameters impact the results. . If you must know, I had to make this. We kept debating on the effectiveness of techniques like SMOTE during my lunch break. Eventually, my curiosity won out and here we are. Does SMOTE work? Keep reading to find out! Or just skip to the conclusion. . For more information about this algorithm, check out the original paper. Or if you’re looking for a visual explanation, this post does a good job. . The findings in this notebook represent observed trends but actual results may vary. Additionally, different datasets may respond differently to SMOTE. These findings are not verified by the FDA. ;) . This work was part of a one month PoC for an Employee Attrition Analytics project at Honeywell International. I presented this notebook at a Honeywell internal data science meetup group and received permission to post it publicly. I would like to thank Matt Pettis (Managing Data Scientist), Nabil Ahmed (Solution Architect), Kartik Raval (Data SME), and Jason Fraklin (Business SME). Without their mentorship and contributions, this project would not have been possible. . A Quick Refresh on Performance Measures . There are lots of performance measures to choose from for classification problems. We’ll look at a few to compare these models. . Accuracy . is the percentage of correctly classified instances. However, if the majority class makes up 99% of the data, then it is easy to get an accuracy of 99% by always predicting the majority class. For this reason, accuracy is not a good measure for imbalanced classification problems. 1 - ACC results in the misclassification error or error rate. . . Balanced Accuracy . on the other hand, gives equal weight to the relative proportions of negative and positive class instances. If a model predicts only one class, the best balanced accuracy it could receive is 50%. 1 - BAC results in the balanced error rate. . . F1 Score . is the harmonic mean of precision and recall. A perfect model has a precision and recall of 1 resulting in an F1 score of 1. For all other models, there exists a tradeoff between precision and recall. F1 is a measure that helps us to judge how much of the tradeoff is worthwhile. . . or . . Setup . # Libraries library(tidyverse) # Data manipulation library(mlr) # Modeling framework library(parallelMap) # Parallelization # Parallelization parallelStartSocket(parallel::detectCores()) # Loading Data source(&quot;prep_EmployeeAttrition.R&quot;) . Defining the Task . As before, we define the Task at hand: predicting attrition up to four weeks out. . tsk_4wk = makeClassifTask(id = &quot;4 week prediction&quot;, data = data %&gt;% select(-c(!! exclude)), target = &quot;Left4wk&quot;, # Must be a factor variable positive = &quot;Left&quot; ) tsk_4wk &lt;- mergeSmallFactorLevels(tsk_4wk) set.seed(5456) ho_4wk &lt;- makeResampleInstance(&quot;Holdout&quot;, tsk_4wk, stratify = TRUE) # Default 1/3rd tsk_train_4wk &lt;- subsetTask(tsk_4wk, ho_4wk$train.inds[[1]]) tsk_test_4wk &lt;- subsetTask(tsk_4wk, ho_4wk$test.inds[[1]]) . Defining the Learners . Here we define 3 separate learner lists. Each contains the model with and without SMOTE. . rate &lt;- 18 neighbors &lt;- 5 logreg_lrns = list( makeLearner(&quot;classif.logreg&quot;, predict.type = &quot;prob&quot;) ,makeSMOTEWrapper(makeLearner(&quot;classif.logreg&quot;, predict.type = &quot;prob&quot;), sw.rate = rate, sw.nn = neighbors)) rpart_lrns = list( makeLearner(&quot;classif.rpart&quot;, predict.type = &quot;prob&quot;) ,makeSMOTEWrapper(makeLearner(&quot;classif.rpart&quot;, predict.type = &quot;prob&quot;), sw.rate = rate, sw.nn = neighbors)) randomForest_lrns = list( makeLearner(&quot;classif.randomForest&quot;, predict.type = &quot;prob&quot;) ,makeSMOTEWrapper(makeLearner(&quot;classif.randomForest&quot;, predict.type = &quot;prob&quot;), sw.rate = rate, sw.nn = neighbors)) . Defining the Resampling Strategy . Here we define the resampling technique. This strategy is implemented repeatedly throughout this notebook. Each time it chooses different records for each fold accounting for some of the variability between the models. . # Define the resampling technique folds = 20 rdesc = makeResampleDesc(&quot;CV&quot;, iters = folds, stratify = TRUE) # stratification with respect to the target . Benchmarking Logistic Regression . First, we’ll consider the logistic regression and evaluate how SMOTE impacts model performance. . # Fit the model logreg_bchmk = benchmark(logreg_lrns, tsk_train_4wk, rdesc, show.info = FALSE, measures = list(acc, bac, auc, f1)) logreg_bchmk_perf &lt;- getBMRAggrPerformances(logreg_bchmk, as.df = TRUE) logreg_bchmk_perf %&gt;% select(-task.id) %&gt;% knitr::kable() . learner.id acc.test.mean bac.test.mean auc.test.mean f1.test.mean . classif.logreg | 0.9290151 | 0.4997191 | 0.8398939 | 0.0000000 | . classif.logreg.smoted | 0.6743728 | 0.7880844 | 0.8234845 | 0.2869752 | . Both models have nearly an identical AUC value of about 0.83. It seems these models effectively trading off accuracy and balanced accuracy. The SMOTE model has a higher balanced accuracy and F1 score of 78.8% and 0.29 respectively (compared to 50% and 0). And the vanilla model has a higher accuracy of 92.9% (compared to 67.4%). . # Visualize results logreg_df_4wk = generateThreshVsPerfData(logreg_bchmk, measures = list(fpr, tpr, mmce, bac, ppv, tnr, fnr, f1)) . ROC Curves . plotROCCurves(logreg_df_4wk) . . Looking at the ROC curves, we see that they intersect but otherwise have similar performance. It is important to note, in practice, we choose a threshold to operate a model. Therefore, the model with a larger area may not be the model with better performance within a limited threshold range. . Precision-Recall Curves . plotROCCurves(logreg_df_4wk, measures = list(tpr, ppv), diagonal = FALSE) . . Here, if you are considering AUC-PR, the vanilla logistic regression does better than the SMOTEd model. Another thing to note is that the positive predictive value (precision) is fairly low for both models. Even though the AUC looked decent at 0.83 there is still a lot of imprecision in these models. Otherwise, the SMOTE model generally does better when recall (TPR) is high and vice verse for the vanilla model. . Threshold Plots . plotThreshVsPerf(logreg_df_4wk, measures = list(fpr, fnr)) . . Threshold plots are common visualizations that help determine an appropriate threshold on which to operate. The FPR and FNR clearly illustrate the opposing tradeoff of each model. However it is difficult to compare these models using FPR and FNR since the imbalanced nature of the data has effectively squished the vanilla logistic model to the far left: slope is zero when the threshold is greater than ≈ 0.4. . plotThreshVsPerf(logreg_df_4wk, measures = list(f1, bac)) . . For our use case, threshold plots for F1 score and balanced accuracy make it easier to identify good thresholds. And while the vanilla logistic regression is still squished to the left, we can compare the performance peaks for the models. For F1, the vanilla model tends to have a higher peak. Whereas for balanced accuracy, SMOTE tends to have a slightly higher peak. Notice that the balanced accuracy for the SMOTEd model centers around the default threshold of 0.5 whereas the F1 score does not. . Confusion Matrices . To calculate the confusion matrices, we’ll train a new model using the full training set and predict against the holdout. Before, we only used the training data and aggregated the performance of the 20 cross-validated folds. We separate the data this way to prevent biasing our operating thresholds for these models. . The training set and holdout are defined at the beginning of this notebook and do not change. However, after tuning the SMOTE parameters, we rerun the cross-validation which may result in changes to the SMOTE model and operating thresholds for both models. . Vanilla Logistic Regression (default threshold) . predicted true Left Stayed -err.- Left 0 67 67 Stayed 0 884 0 -err.- 0 67 67 acc bac auc f1 0.9295478 0.5000000 0.8041298 0.0000000 . If you just look at accuracy (93%), this model performs great! But it is useless for the business. This model predicted that 0 employees would leave in the next 4 weeks but actually 67 left. This is why we need balanced performance measures like balanced accuracy for imbalanced classification problems. The balanced accuracy of 50% clearly illustrates the problem of this model. . SMOTE Logistic Regression (default threshold) . predicted true Left Stayed -err.- Left 56 11 11 Stayed 277 607 277 -err.- 277 11 288 acc bac auc f1 0.6971609 0.7612362 0.8177382 0.2800000 . This is the first evidence that SMOTE works. We have a more balanced model (76.1% balanced accuracy compared to 50%) that might actually be useful for the business. It narrows the pool of employees at risk of attrition from 951 down to 333 while capturing 83.6% of employees that actually left. If this were the only information available, then SMOTE does appear to result in a better model. . However the AUC is similar for both models indicating similar performance. As mentioned earlier, we can operate these models at different thresholds. . Tuning the Operating Threshold . The following code tunes the operating threshold for each model: . metric &lt;- f1 logreg_thresh_vanilla &lt;- tuneThreshold( getBMRPredictions(logreg_bchmk ,learner.ids =&quot;classif.logreg&quot; ,drop = TRUE) ,measure = metric) logreg_thresh_SMOTE &lt;- tuneThreshold( getBMRPredictions(logreg_bchmk ,learner.ids =&quot;classif.logreg.smoted&quot; ,drop = TRUE) ,measure = metric) . Here we’ve tuned these models using the F1 measure but we could have easily used a different metric. . Vanilla Logistic Regression (tuned threshold) . predicted true Left Stayed -err.- Left 29 38 38 Stayed 130 754 130 -err.- 130 38 168 acc bac auc f1 0.8233438 0.6428885 0.8041298 0.2566372 . SMOTE Logistic Regression (tuned threshold) . predicted true Left Stayed -err.- Left 39 28 28 Stayed 164 720 164 -err.- 164 28 192 acc bac auc f1 0.7981073 0.6982846 0.8177382 0.2888889 . Setting the tuned operating threshold results in two very similar models! Depending on the run, there might be a slight benefit to the SMOTEd model, but not enough to say with confidence. . But perhaps SMOTE just needs some tuning. . Tuning SMOTE . The SMOTE algorithm is defined by the parameters rate and nearest neighbors. Rate defines how much to oversample the minority class. Nearest neighbors defines how many nearest neighbors to consider. Tuning these should result in better model performance. . logreg_ps = makeParamSet( makeIntegerParam(&quot;sw.rate&quot;, lower = 8L, upper = 28L) ,makeIntegerParam(&quot;sw.nn&quot;, lower = 2L, upper = 8L) ) ctrl = makeTuneControlIrace(maxExperiments = 400L) logreg_tr = tuneParams(logreg_lrns[[2]], tsk_train_4wk, rdesc, list(f1, bac), logreg_ps, ctrl) logreg_lrns[[2]] = setHyperPars(logreg_lrns[[2]], par.vals=logreg_tr$x) # Fit the model logreg_bchmk = benchmark(logreg_lrns, tsk_train_4wk, rdesc, show.info = FALSE, measures = list(acc, bac, auc, f1)) logreg_thresh_vanilla &lt;- tuneThreshold( getBMRPredictions(logreg_bchmk ,learner.ids =&quot;classif.logreg&quot; ,drop = TRUE), measure = metric) logreg_thresh_SMOTE &lt;- tuneThreshold( getBMRPredictions(logreg_bchmk ,learner.ids =&quot;classif.logreg.smoted&quot; ,drop = TRUE), measure = metric) . Vanilla Logistic Regression (tuned threshold) . predicted true Left Stayed -err.- Left 35 32 32 Stayed 145 739 145 -err.- 145 32 177 acc bac auc f1 0.8138801 0.6791805 0.8041298 0.2834008 . SMOTE Logistic Regression (tuned threshold and SMOTE) . predicted true Left Stayed -err.- Left 42 25 25 Stayed 182 702 182 -err.- 182 25 207 acc bac auc f1 0.7823344 0.7104917 0.8105795 0.2886598 . If we account for resampling variance, the tuned SMOTE makes little difference. Perhaps the initial SMOTE parameters close enough to the optimal settings. This table shows how they changed: .   Rate Nearest Neighbors . Initial | 18 | 5 | . Tuned | 9 | 2 | . The rate decreased by 9 and the number of nearest neighbors decreased by 3. . After running this code multiple times, SMOTE generally produces models with higher balanced accuracy but lower accuracy. In terms of AUC and F1, it is harder to tell. Either way, even if SMOTE is tuned, observed performance increases are small compared to a vanilla logistic model with a tuned operating threshold. These results may also depend on the data itself. A different dataset intended to solve another imbalanced classification problem may have different results using SMOTE with logistic regression. . Benchmarking Decision Tree . # Fit the model rpart_bchmk = benchmark(rpart_lrns, tsk_train_4wk, rdesc, show.info = FALSE, measures = list(acc, bac, auc, f1)) rpart_bchmk_perf &lt;- getBMRAggrPerformances(rpart_bchmk, as.df = TRUE) rpart_bchmk_perf %&gt;% select(-task.id) %&gt;% knitr::kable() . learner.id acc.test.mean bac.test.mean auc.test.mean f1.test.mean . classif.rpart | 0.9290539 | 0.5888060 | 0.6995163 | 0.2594619 | . classif.rpart.smoted | 0.7491146 | 0.7771476 | 0.8605376 | 0.3113137 | . Let’s be honest, the SMOTEd logistic regression was lackluster. But for the decision tree model, SMOTE increases AUC by 0.16. Both flavors have similar F1 scores; otherwise we see the same tradeoff between accuracy and balanced accuracy as in the logistic regression. . # Visualize results rpart_df_4wk = generateThreshVsPerfData(rpart_bchmk, measures = list(fpr, tpr, mmce, bac, ppv, tnr, fnr, f1)) . ROC Curves . plotROCCurves(rpart_df_4wk) . . It’s easy to see that SMOTE has a higher AUC than the vanilla model, but since the lines cross, each perform better within certain operating thresholds. . Precision-Recall Curves . plotROCCurves(rpart_df_4wk, measures = list(tpr, ppv), diagonal = FALSE) . . The vanilla model scores much higher on precision (PPV) but declines much more quickly as recall increases. SMOTE is more precise when recall (TPR) is greater than ≈ 0.75. Additionally, notice the straight lines, likely, there are no data in these regions making each model only viable for half the PR Curve. . Threshold Plots . plotThreshVsPerf(rpart_df_4wk, measures = list(fpr, fnr)) . . The nearly vertical slopes of these threshold plots represent the straight lines on the PR Curve plot. . plotThreshVsPerf(rpart_df_4wk, measures = list(f1, bac)) . . If we’re concerned primarily with balanced accuracy, SMOTE is clearly better at all thresholds. For the F1 score however, it depends on the operating threshold of the model. Notice balanced accuracy is once again centered around the default threshold of 0.5 and the F1 measure is not. The F1 performance to threshold pattern is roughly opposite for the two flavors of decision trees. . Confusion Matrices . Vanilla Decision Tree (default threshold) . predicted true Left Stayed -err.- Left 10 57 57 Stayed 7 877 7 -err.- 7 57 64 acc bac auc f1 0.9327024 0.5706676 0.7061694 0.2380952 . Using the default threshold, the vanilla decision tree manages to identify some employee attrition. In face, its accuracy of 93.3% is higher than the baseline case of always predicting the majority class (93%). It is relatively precise (0.59) but has low recall (0.15). Overall accuracy is high (93.3%), but the model is not very balanced (57.1%). . SMOTE Decision Tree (default threshold) . predicted true Left Stayed -err.- Left 55 12 12 Stayed 232 652 232 -err.- 232 12 244 acc bac auc f1 0.7434280 0.7792260 0.8338117 0.3107345 . The SMOTE Decision Tree does a much better job of capturing employees that left (82% compared to 15%) but at the cost of precision. The model identifies 287 when only 55 from that group actually leave. Still this model is more useful to the business than the vanilla decision tree at the default threshold. . Tuning the Operating Threshold . rpart_thresh_vanilla &lt;- tuneThreshold( getBMRPredictions(rpart_bchmk ,learner.ids =&quot;classif.rpart&quot; ,drop = TRUE), measure = metric) rpart_thresh_SMOTE &lt;- tuneThreshold( getBMRPredictions(rpart_bchmk ,learner.ids =&quot;classif.rpart.smoted&quot; ,drop = TRUE), measure = metric) . As before, we’ll be using the F1 measure. . Vanilla Decision Tree (tuned threshold) . predicted true Left Stayed -err.- Left 22 45 45 Stayed 26 858 26 -err.- 26 45 71 acc bac auc f1 0.9253417 0.6494732 0.7061694 0.3826087 . Once we tune the threshold, the vanilla decision tree model performs much better–identifying more employees that leave with relatively high precision. The F1 score increases from 0.238 to 0.383. . SMOTE Decision Tree (tuned threshold) . predicted true Left Stayed -err.- Left 32 35 35 Stayed 80 804 80 -err.- 80 35 115 acc bac auc f1 0.8790747 0.6935571 0.8338117 0.3575419 . Changing the operating threshold for the SMOTEd decision tree results in a 13.6% higher accuracy, 8.57% lower balanced accuracy, and higher F1 measure of 4.68%. . These changes to the operating threshold result in a similar F1 performance for both flavors of decision tree (0.358 compared to 0.383). . Tuning SMOTE . rpart_ps = makeParamSet( makeIntegerParam(&quot;sw.rate&quot;, lower = 8L, upper = 28L) ,makeIntegerParam(&quot;sw.nn&quot;, lower = 2L, upper = 8L) ) ctrl = makeTuneControlIrace(maxExperiments = 200L) rpart_tr = tuneParams(rpart_lrns[[2]], tsk_train_4wk, rdesc, list(f1, bac), rpart_ps, ctrl) rpart_lrns[[2]] = setHyperPars(rpart_lrns[[2]], par.vals=rpart_tr$x) . # Fit the model rpart_bchmk = benchmark(rpart_lrns, tsk_train_4wk, rdesc, show.info = FALSE, measures = list(acc, bac, auc, f1)) rpart_thresh_vanilla &lt;- tuneThreshold( getBMRPredictions(rpart_bchmk ,learner.ids =&quot;classif.rpart&quot; ,drop = TRUE), measure = metric) rpart_thresh_SMOTE &lt;- tuneThreshold( getBMRPredictions(rpart_bchmk ,learner.ids =&quot;classif.rpart.smoted&quot; ,drop = TRUE), measure = metric) . Vanilla Decision Tree (tuned threshold) . predicted true Left Stayed -err.- Left 23 44 44 Stayed 35 849 35 -err.- 35 44 79 acc bac auc f1 0.9169295 0.6518454 0.7061694 0.3680000 . SMOTE Decision Tree (tuned threshold and SMOTE) . predicted true Left Stayed -err.- Left 32 35 35 Stayed 80 804 80 -err.- 80 35 115 acc bac auc f1 0.8790747 0.6935571 0.7885628 0.3575419 . Tuning SMOTE for the decision tree changed the accuracy from 87.9% to 87.9% and the balanced accuracy from 69.4% to 69.4%. The following table shows how the rate and number of nearest neighbors changed: .   Rate Nearest Neighbors . Initial | 18 | 5 | . Tuned | 15 | 7 | . The rate decreased by 3 and the number of nearest neighbors increased by 2. . Given our data, SMOTE for decision trees seems to offer real performance increases to the model. That said, the performance increases are largely via tradeoff between accuracy and balanced accuracy. Setting the operating threshold for the vanilla model results in a similarly performant model. However, we need to consider that identifying rare events is our primary concern. SMOTE allows us to operate with increased performance when high recall is important. . Benchmarking randomForest . # Fit the model randomForest_bchmk = benchmark(randomForest_lrns, tsk_train_4wk, rdesc, show.info = FALSE, measures = list(acc, bac, auc, f1)) randomForest_bchmk_perf &lt;- getBMRAggrPerformances(randomForest_bchmk, as.df = TRUE) randomForest_bchmk_perf %&gt;% select(-task.id) %&gt;% knitr::kable() . learner.id acc.test.mean bac.test.mean auc.test.mean f1.test.mean . classif.randomForest | 0.9310609 | 0.5878136 | 0.8942103 | 0.2674242 | . classif.randomForest.smoted | 0.8868540 | 0.7553178 | 0.8875052 | 0.4339340 | . For the randomForest models, we see similar patters as for the logistic regression and decision tress. There is a trade off between accuracy and balanced accuracy. Unlike the decision tree models, SMOTE does not improve AUC for SMOTE randomForest (both are ≈ 0.89). . # Visualize results randomForest_df_4wk = generateThreshVsPerfData(randomForest_bchmk, measures = list(fpr, tpr, mmce, bac, ppv, tnr, fnr, f1)) . ROC Curves . plotROCCurves(randomForest_df_4wk) . . Both models cross multiple times showing either model is likely good for most thresholds. . Precision-Recall Curves . plotROCCurves(randomForest_df_4wk, measures = list(tpr, ppv), diagonal = FALSE) . . This PR-Curve shows more distinctly that SMOTE generally performs better when recall is high, whereas the vanilla model generally performs better when recall is lower. . Threshold Plots . plotThreshVsPerf(randomForest_df_4wk, measures = list(fpr, fnr)) . . plotThreshVsPerf(randomForest_df_4wk, measures = list(f1, bac)) . . Interestingly, the SMOTE randomForest does not center balanced accuracy around the default threshold; rather F1 is centered on the 0.5 threshold. Otherwise we see that SMOTE produces a higher peak for balanced accuracy but lower for F1. Additionally, the vanilla model is still squished to the left due to its class imbalance. . Confusion Matrices . Vanilla randomForest (default threshold) . predicted true Left Stayed -err.- Left 15 52 52 Stayed 7 877 7 -err.- 7 52 59 acc bac auc f1 0.9379600 0.6079810 0.8559718 0.3370787 . As far as vanilla models go, and given the default threshold, the randomForest performs the best. That said, this model captures very few employees that leave. . SMOTE randomForest (default threshold) . predicted true Left Stayed -err.- Left 34 33 33 Stayed 78 806 78 -err.- 78 33 111 acc bac auc f1 0.8832808 0.7096137 0.8686263 0.3798883 . The SMOTEd randomForest also does well. The accuracy is high and manages a good balanced accuracy. . Tuning the Operating Threshold . randomForest_thresh_vanilla &lt;- tuneThreshold( getBMRPredictions(randomForest_bchmk ,learner.ids =&quot;classif.randomForest&quot; ,drop = TRUE), measure = metric) randomForest_thresh_SMOTE &lt;- tuneThreshold( getBMRPredictions(randomForest_bchmk ,learner.ids =&quot;classif.randomForest.smoted&quot; ,drop = TRUE), measure = metric) . As before, we’ll be using the F1 measure. . Vanilla randomForest (tuned threshold) . predicted true Left Stayed -err.- Left 38 29 29 Stayed 78 806 78 -err.- 78 29 107 acc bac auc f1 0.8874869 0.7394644 0.8559718 0.4153005 . SMOTE randomForest (tuned threshold) . predicted true Left Stayed -err.- Left 36 31 31 Stayed 81 803 81 -err.- 81 31 112 acc bac auc f1 0.8822292 0.7228422 0.8686263 0.3913043 . At the tuned threshold, the performance of both flavors perform better and are once again very similar. Depending on the run, SMOTE will have a higher balanced accuracy, but otherwise there is little difference between the models. . Tuning SMOTE . randomForest_ps = makeParamSet( makeIntegerParam(&quot;sw.rate&quot;, lower = 8L, upper = 28L) ,makeIntegerParam(&quot;sw.nn&quot;, lower = 2L, upper = 8L) ) ctrl = makeTuneControlIrace(maxExperiments = 200L) randomForest_tr = tuneParams(randomForest_lrns[[2]], tsk_train_4wk, rdesc, list(f1, bac), randomForest_ps, ctrl) randomForest_lrns[[2]] = setHyperPars(randomForest_lrns[[2]], par.vals=randomForest_tr$x) . # Fit the model randomForest_bchmk = benchmark(randomForest_lrns, tsk_train_4wk, rdesc, show.info = FALSE, measures = list(acc, bac, auc, f1)) randomForest_thresh_vanilla &lt;- tuneThreshold( getBMRPredictions(randomForest_bchmk ,learner.ids =&quot;classif.randomForest&quot; ,drop = TRUE), measure = metric) randomForest_thresh_SMOTE &lt;- tuneThreshold( getBMRPredictions(randomForest_bchmk ,learner.ids =&quot;classif.randomForest.smoted&quot; ,drop = TRUE), measure = metric) . Vanilla randomForest (tuned threshold) . predicted true Left Stayed -err.- Left 40 27 27 Stayed 101 783 101 -err.- 101 27 128 acc bac auc f1 0.8654048 0.7413808 0.8527470 0.3846154 . SMOTE randomForest (tuned threshold and SMOTE) . predicted true Left Stayed -err.- Left 37 30 30 Stayed 98 786 98 -err.- 98 30 128 acc bac auc f1 0.8654048 0.7206895 0.8637384 0.3663366 . Given this data, tuning SMOTE does not seem to improve performance. The following table shows how the parameters for SMOTE changed during the tuning process: .   Rate Nearest Neighbors . Initial | 18 | 5 | . Tuned | 18 | 3 | . The rate did not change and the number of nearest neighbors decreased by 2. . Given this data for randomForest, SMOTE does little to improve model performance. At optimized operating thresholds, both flavors end up with very similar accuracy and balanced accuracy. There does appear to be some benefit using SMOTE where recall is high and precision is low, however the business may not want to throw such a large net in order to capture all of the employees that leave. Practically speaking, SMOTE did not improve the performance for this problem when using randomForest. . parallelStop() . Stopped parallelization. All cleaned up. . Conclusion . Given this data, SMOTE improved AUC of the decision tree model but offered little improvement for logistic regression or randomForest. Otherwise, SMOTE offered a way to trade accuracy for balanced accuracy. For our problem of employee attrition, this trade off is worth it to continue using SMOTE. Even when operating thresholds are optimized, there is–at worst–no change in the performance of the models. That said, the ideal solution might be an ensemble of SMOTE and vanilla models at operating thresholds suited for their flavor. . This notebook shows SMOTE impacts models differently, a finding supported by Experimental Perspectives on Learning from Imbalanced Data. They also found, while generally beneficial, SMOTE often did not perform as well as simple random undersampling–something we might try in a future notebook. A different paper, SMOTE for high-dimensional class-imbalanced data, found that for high-dimensional data, SMOTE is beneficial but only after variable selection is performed. The employee attrition problem featured here does not have high-dimensional data, however it is useful to consider how feature selection may impact the calculated Euclidean distance used in the SMOTE algorithm. If we gather more features, it may be beneficial to perform more rigorous feature selection before SMOTE to improve model performance. .",
            "url": "https://patdmob.github.io/blog/tutorial/r%20programming/imbalanced%20classification/2018/05/16/Vanilla-vs-SMOTE.html",
            "relUrl": "/tutorial/r%20programming/imbalanced%20classification/2018/05/16/Vanilla-vs-SMOTE.html",
            "date": " • May 16, 2018"
        }
        
    
  
    
        ,"post3": {
            "title": "Imbalanced Classification with mlr",
            "content": "Outline . Introduction | Data | Model Development Model Setup | Tune Hyperparameters | Measuring Performance | . | Results | Interpretability | Production Model | . Introduction . Background . This notebook presents a reference implementation of an imbalanced data problem, namely predicting employee attrition. We’ll use mlr, a package designed to provide an infrastructure for Machine Learning in R. Additionally, there is a companion post which investigates the effectiveness of SMOTE compared to non-SMOTE models. . Unfortunately, the data is proprietary and we cannot disclose the details of the data with outside parties. But the field represented by this data sees 20% annual turnover in employees. Each employee separation costs roughly $20K. Meaning, a 25% reduction to employee attrition results in an annual savings of over $400K. . Using a predictive model, HR organizations can build on the data of today to anticipate the events of tomorrow. This forward notice offers the opportunity to respond by developing a tailored retention strategy to retain employees before they jump ship. . This work was part of a one month PoC for an Employee Attrition Analytics project at Honeywell International. I presented this notebook at a Honeywell internal data science meetup group and received permission to post it publicly. I would like to thank Matt Pettis (Managing Data Scientist), Nabil Ahmed (Solution Architect), Kartik Raval (Data SME), and Jason Fraklin (Business SME). Without their mentorship and contributions, this project would not have been possible. . Setup . # Libraries library(tidyverse) # Data manipulation library(mlr) # Modeling framework library(parallelMap) # Parallelization library(rpart.plot) # Decision Tree Visualization library(parallel) # To detect # of cores # Parallelization parallelStartSocket(detectCores()) # Loading Data source(&quot;prep_EmployeeAttrition.R&quot;) . Data . Since the primary purpose of this notebook is modeling employee attrition, we won’t go into the data preprocessing steps; but they involved sql querying, reformatting, cleaning, filtering, and variable creation. . The loaded data represents a snapshot in time, aggregating 52-weeks of history into performance and summary metrics. To build a predictive model, we choose the end of this 52-week period to be at least 4 weeks in the past. Finally we created a variable indicating if an employee left in the following four week period. . To get summary statistics within mlr, you can use summarizeColumns(): . summarizeColumns(data) . Output not shown for proprietary reasons. . Data Structure . Employees: 2852 Model Features: 15 Target Variable: Left4wk . data %&gt;% summarise(`Total Employees` = n(), `Attrition Count` = sum(Left4wk==&quot;Left&quot;), `Attrition Percent` = mean(Left4wk==&quot;Left&quot;)) %&gt;% knitr::kable() . Total Employees Attrition Count Attrition Percent . 2852 | 201 | 0.0704769 | . Considerations . Concern: Employee attrition is a imbalanced classification problem, meaning that the group of interest is relatively rare. This can cause models to overclassify the majority group in an effort to get better accuracy. After all, if predict every employee will stay, we can get an accuracy of 93%, but this is not a useful model. Solution: There are two general methods to overcome this issue: sampling techniques and skew-insensitive classifiers. Synthetic Minority Oversampling TEchnique (SMOTE) is a sampling technique well suited for employee attrition. We’ll use this method to create a balanced model for predicting employee attrition. . Model Development . We’ll use mlr to help us setup the models, run cross-validation, perform hyperparameter tuning, and measure performance of the final models. . Model Setup . Defining the Task . Just as dplyr provides a grammar for manipulating data frames, mlr provides a grammar for data modeling. The first grammatical object is the task. A task is an object that defines at minimum the data and the target. . For this project, our task is to predict employee attrition 4 weeks out. Here we also create a holdout test and train dataset for each task. . # Defining Task tsk_4wk &lt;- makeClassifTask(id = &quot;4 week prediction&quot;, data = data %&gt;% select(-c(!! exclude)), target = &quot;Left4wk&quot;, # Must be a factor variable positive = &quot;Left&quot; ) tsk_4wk &lt;- mergeSmallFactorLevels(tsk_4wk) # Creating 4 week holdout datasets ho_4wk &lt;- makeResampleInstance(&quot;Holdout&quot;, tsk_4wk, stratify = TRUE) # Default 1/3rd tsk_train_4wk &lt;- subsetTask(tsk_4wk, ho_4wk$train.inds[[1]]) tsk_test_4wk &lt;- subsetTask(tsk_4wk, ho_4wk$test.inds[[1]]) . Note that the target variable needs to be a factor variable. For Python users, a factor variable is a data type within R specific for representing categorical variables. It can represent information as ordered (e.g. small, medium, large) or unordered (e.g. red, green, blue) and models can take advantage of these relationships. Variables in this dataset were reformatted to factor as part of the preprocessing. . train_target &lt;- table(getTaskTargets(tsk_train_4wk)) train_target . Left Stayed 134 1767 . Again, we are dealing with an imbalanced classification problem. After splitting the data, our training sample has 134 employees that left out of 1901 total employees. . We’ll use the SMOTE technique described earlier to synthetically generate more employees that left. This will result in a more balanced dataset for training. However, since the test set is still imbalanced, we need to consider balanced performance measures like balanced accuracy and F1 when evaluating and tuning our models. . Defining the Learners . Next, we’ll use three different models to predict employee attrition. The advantage of this approach is that some models perform better on certain problems. By using a few different models were more likely to use a good model for this problem. Also, while some models might provide a better answer, they can be more difficult to explain how or why they work. By using multiple models, we should be able to provide both a predictive and explainable answer. The best of both worlds. . Here we define the three models we will use to predict employee attrition. Notice they are wrapped in a SMOTE function. . lrns &lt;- list( makeSMOTEWrapper(makeLearner(&quot;classif.logreg&quot;, predict.type = &quot;prob&quot;), sw.rate = 18, sw.nn = 8), makeSMOTEWrapper(makeLearner(&quot;classif.rpart&quot;, predict.type = &quot;prob&quot;), sw.rate = 18, sw.nn = 8), makeSMOTEWrapper(makeLearner(&quot;classif.randomForest&quot;, predict.type = &quot;prob&quot;), sw.rate = 18, sw.nn = 8)) . Pause: Let’s review the process flow . . The order of operations is important. If you SMOTE before splitting the data, then you’ve effectively polluted the training set with information from the test set! mlr has a smote() function, but that works by redefining the task and will happen before the resampling split. Therefore, we wrapped the smote around the learner which is applied after the resampling split. . Defining the Resampling Strategy . To ensure extensible models to new data, we’ll use cross-validation to guard against overfitting. . folds &lt;- 20 rdesc &lt;- makeResampleDesc(&quot;CV&quot;, iters = folds, stratify = TRUE) # stratification with respect to the target . We use 20 folds here, but I’d recommend fewer during the exploratory phase since more folds require more computation. . Model Cross-validation . Let’s run a quick cross-validation iteration to see how the models perform before tuning them. . bchmk &lt;- benchmark(lrns, tsk_train_4wk, rdesc, show.info = FALSE, measures = list(acc, bac, auc, f1)) bchmk_perf &lt;- getBMRAggrPerformances(bchmk, as.df = TRUE) bchmk_perf %&gt;% select(-task.id) %&gt;% knitr::kable() . learner.id acc.test.mean bac.test.mean auc.test.mean f1.test.mean . classif.logreg.smoted | 0.6500867 | 0.7478255 | 0.7899618 | 0.2610063 | . classif.rpart.smoted | 0.7428074 | 0.7618259 | 0.8358594 | 0.3027377 | . classif.randomForest.smoted | 0.8695530 | 0.7185079 | 0.8700374 | 0.3754485 | . Not bad; the best model has an accuracy of 87%. By looking at different, sometimes competing, measures we can better gauge the performance of the models. Above we’ve computed accuracy, balanced accuracy, AUC, and F1. . Shown below are boxplots showing the performance measure distribution for each of the 20 cross-validation iterations. All the models seem to perform reasonably well when applied to new data. . plotBMRBoxplots(bchmk, measure = acc) . . However when we look at balanced accuracy, we see a performance drop. Balanced accuracy gives equal weight to the relative proportion of each class (left vs stayed) resulting in a more difficult metric. . plotBMRBoxplots(bchmk, measure = bac) . . With this we’ve built some models, but now we need to refine them. Let’s see if we can improve performance by tuning the hyperparameters. . Tune Hyperparameters . Tuning works by optimizing the cross-validated aggregated performance metric like accuracy or balanced accuracy. This mitigates overfitting because each fold needs to perform reasonable well as to not pull down the aggregation. For this imbalanced data problem, we’ll tune using both F1 score and balanced accuracy. . The SMOTE algorithm is defined by the parameters rate and nearest neighbors. Rate defines how much to oversample the minority class. Nearest neighbors defines how many nearest neighbors to consider. For more information about this algorithm check out this post and the original paper. Since SMOTE has tunable hyperparameters, we’ll tune the logistic regression too. In addition, decision trees and randomForests have model specific hyperparameters. If you’re unsure what hyperparameters are tunable, us getParamSet(&lt;learner&gt;) to find out. . # Logistic logreg_ps &lt;- makeParamSet( makeIntegerParam(&quot;sw.rate&quot;, lower = 8L, upper = 28L) ,makeIntegerParam(&quot;sw.nn&quot;, lower = 2L, upper = 8L) ) # DecisionTree rpart_ps &lt;- makeParamSet( makeIntegerParam(&quot;sw.rate&quot;, lower = 8L, upper = 28L) ,makeIntegerParam(&quot;sw.nn&quot;, lower = 2L, upper = 8L) ,makeIntegerParam(&quot;minsplit&quot;,lower = 10L, upper = 50L) ,makeIntegerParam(&quot;minbucket&quot;, lower = 5L, upper = 70L) ,makeNumericParam(&quot;cp&quot;, lower = 0.005, upper = .05) ) # RandomForest randomForest_ps &lt;- makeParamSet( makeIntegerParam(&quot;sw.rate&quot;, lower = 8L, upper = 28L) ,makeIntegerParam(&quot;sw.nn&quot;, lower = 2L, upper = 8L) ,makeIntegerParam(&quot;ntree&quot;, lower = 50L, upper = 600L) ,makeIntegerParam(&quot;mtry&quot;, lower = 1L, upper = 20L) ,makeIntegerParam(&quot;nodesize&quot;, lower = 4L, upper = 50L) ) . After defining the bounds of each hyperparameter, we define the tuning control to intelligently search the space for an optimal hyperparameter set. Irace and MBO are different methods for optimizing hyperparameters. After tuning each model, we update the learner with the optimal configuration for future training. . # ctrl = makeTuneControlMBO(budget=200) ctrl &lt;- makeTuneControlIrace(maxExperiments = 400L) logreg_tr &lt;- tuneParams(lrns[[1]], tsk_train_4wk, rdesc, list(f1), logreg_ps, ctrl) lrns[[1]] &lt;- setHyperPars(lrns[[1]], par.vals=logreg_tr$x) rpart_tr &lt;- tuneParams(lrns[[2]], tsk_train_4wk, rdesc, list(f1), rpart_ps, ctrl) lrns[[2]] &lt;- setHyperPars(lrns[[2]], par.vals=rpart_tr$x) randomForest_tr &lt;- tuneParams(lrns[[3]], tsk_train_4wk, rdesc, list(f1), randomForest_ps, ctrl) lrns[[3]] &lt;- setHyperPars(lrns[[3]], par.vals=randomForest_tr$x) . It’s important to know that for each iteration of the tuning process, a full cross-validation resampling of 20 folds occurs. . Measuring Performance . Now that we’ve tuned our hyperparameters, we need to train on all training data and assess model performance against the holdout. This will give us some idea how the model will perform on new data. . bchmk &lt;- benchmark(lrns, tsk_4wk, ho_4wk, show.info = FALSE, measures = list(acc, bac, auc, f1)) bchmk_perf &lt;- getBMRAggrPerformances(bchmk, as.df = TRUE) bchmk_perf %&gt;% select(-task.id) %&gt;% knitr::kable() . learner.id acc.test.mean bac.test.mean auc.test.mean f1.test.mean . classif.logreg.smoted | 0.7108307 | 0.8099716 | 0.8424056 | 0.3107769 | . classif.rpart.smoted | 0.8422713 | 0.7220403 | 0.7973847 | 0.3421053 | . classif.randomForest.smoted | 0.8811777 | 0.7636591 | 0.9047494 | 0.4263959 | . One advantage of using mlr’s benchmark() function is that we can create easy comparisons between the three models. Here is the traditional Area Under the Curve (ROC) visualizing one measure of classification performance. The model performs better as the curve stretches towards the upper left thereby maximizing the area. . df_4wk &lt;- generateThreshVsPerfData(bchmk, measures = list(fpr, tpr, mmce, ppv, tnr, fnr), task.id = &#39;4 week prediction&#39;) plotROCCurves(df_4wk) + ggtitle(&quot;Four week attrition model ROC curves&quot;) . . Right now, we are testing the model against the holdout. But after we finish modeling, we’ll train a model using all the data. To understand how well the model integrates new data, we’ll create the learning curve for various measures of performance. . rs_cv5 &lt;- makeResampleDesc(&quot;CV&quot;, iters = 5, stratify = TRUE) lc_4wk &lt;- generateLearningCurveData(learners = lrns, task = tsk_4wk, percs = seq(0.2, 1, by = 0.2), measures = list(acc, bac, auc, f1), resampling = rs_cv5, stratify = TRUE, show.info = FALSE) . plotLearningCurve(lc_4wk, facet.wrap.ncol = 2) + ggtitle(&quot;Four week prediction learning curve&quot;) . . These plots show that the model may benefit from additional data but with decreasing marginal gains. If we want better performance, more data will only help so much–we’ll need better features. . Results . Confusion Matrices . Logistic . predicted true Left Stayed -err.- Left 58 9 9 Stayed 261 623 261 -err.- 261 9 270 acc bac f1 0.7160883 0.7852114 0.3005181 . Decision Tree . predicted true Left Stayed -err.- Left 39 28 28 Stayed 122 762 122 -err.- 122 28 150 acc bac f1 0.8422713 0.7220403 0.3421053 . randomForest . predicted true Left Stayed -err.- Left 42 25 25 Stayed 83 801 83 -err.- 83 25 108 acc bac f1 0.8864353 0.7664871 0.4375000 . These results were computed by running each model on the holdout dataset to simulate new data. Therefore, we should expect similar outcomes from a live implemented production model. Since the randomForest performed the best, we’ll use this model to train our production model but we could also create an ensemble using all three. . Interpretability . There are many ways to extract information from the results of a predictive model which could be valuable to the business. One simple way is to simply use the coefficients from the logistic regression to show any linear trends. . summary(getLearnerModel(mdl_4wk_logistic, more.unwrap = TRUE)) . Output not shown for proprietary reasons. . We can also use a decision tree to visualize how the model works and potential reasons why people leave. . rpart.plot(getLearnerModel(mdl_4wk_decisionTree, more.unwrap=TRUE), extra=104, box.palette=&quot;RdGy&quot;, branch.lty=3, shadow.col=&quot;gray&quot;) . Output not shown for proprietary reasons. . Feature importance plots can also provide valuable insight into how models work. The following code uses a method called permutation feature importance which measures the impact of randomly shuffling the values of a feature. . impt_4wk &lt;- generateFilterValuesData(tsk_4wk, method = &quot;permutation.importance&quot;, imp.learner = lrns[[3]], measure = mmce) plotFilterValues(impt_4wk) + ggtitle(&quot;Feature Importance: 4 Week Prediction&quot;) . Output not shown for proprietary reasons. . Many other methods exist to gain interpretability from blackbox models. A few such methods are SHAP and LIME. Additionally, we can feed the results of these models into a clustering algorithm to group similar types of attrition. If distinct groups emerge, we can create profiles and describe what defines each group. . Production Model . Finally, we train on all the data to get a model to use on real world data. . mdl_4wk_final &lt;- train(lrns[[3]], tsk_4wk) . If we were to deploy this model, we’d continue by setting up a model monitoring framework. Part of this consists of tests to alert on changes to: . Data Continues to flow properly (both input and output) | Inputs are statistically similar to training data | . | Model Performance | Computational load (i.e. is the model taking too long to run for the service?) | . | . For a more detailed list of tests for machine learning production systems, check out the paper by Google researchers, “What’s your ML Test Score? A rubric for ML production systems”. . parallelStop() . Stopped parallelization. All cleaned up. .",
            "url": "https://patdmob.github.io/blog/tutorial/r%20programming/imbalanced%20classification/2018/05/15/Imbalanced-Classification-with-mlr.html",
            "relUrl": "/tutorial/r%20programming/imbalanced%20classification/2018/05/15/Imbalanced-Classification-with-mlr.html",
            "date": " • May 15, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Life has it’s ups and downs. But I’ve always liked to learn. It’s fun to discover the world and how it works. It’s always enjoyable to Netflix and chill but so much more satifying to Make something. Part of my life is maintaining a balance between these two. . I took a two-year hiatus from writing posts. But I’ve found my spark again so you should see much more to come. Topics might be all over the place; I took this space too serously last time. So now I’ll write whatever takes my fancy: perhaps topics like electronics, Raspberry Pi, data science, or economics. . I program in mostly Python these days but I’m not committing to anything. You’ll see older posts in R. And as I learn and work more with microprocessors and microcontrollers you might see something else. . Thanks and I hope you enjoy. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://patdmob.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://patdmob.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}